[{"categories":null,"contents":" Most queries are tested on my onprem homelab servers. Sometimes rights might be limited for example in SplunkCloud for \u0026ldquo;| rest\u0026rdquo; queries.\nUsing tstats is faster to search in large datapools.\nTroubleshooting: When troubleshooting I mostly start with this query:\nindex=_internal log_level=ERROR source=\u0026#34;/opt/splunk/var/log/splunk/splunkd.log\u0026#34; Errorcheck Splunkd.log within Splunk GUI\nindex=\u0026#34;index_name\u0026#34; host=\u0026#34;host_name\u0026#34; | sort sourcetype, _time | streamstats current=f last(_time) as prev_time by sourcetype | eval gap=_time - prev_time | where gap \u0026gt; 3600 | eval gap_in_hours=round(gap/3600,2) | eval prev_time_str=strftime(prev_time, \u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;) | table sourcetype _time prev_time prev_time_str gap gap_in_hours Check for gaps in log ingestion and return results over 1 hour gaps. Troubleshooting for false positive due to unreliable log ingestion of a logtype while other logtypes indicated ongoing log ingestion.\nReporting: When checking the health status of the environment, I use the following queries to provide insight:\nGeneral | rest splunk_server=* count=1 /services/server/info | table version host Check Splunk version\n| rest /services/server/info | eval LastStartupTime=strftime(startup_time, \u0026#34;%Y/%m/%d %H:%M:%S\u0026#34;) | eval timenow=now() | eval daysup = round((timenow - startup_time) / 86400,0) | eval Uptime = tostring(daysup) + \u0026#34; Days\u0026#34; | table splunk_server LastStartupTime Uptime List server uptime\n| rest /services/authentication/users splunk_server=local | fields title roles email | rename title as username List users and their roles\nLogsources | tstats values(host) as recent_host where index=* by host, index, sourcetype | search NOT [| inputlookup monitored_systems.csv | fields host | rename host as recent_host] | table recent_host, index, sourcetype Overview of non-monitored hosts: Using a lookupfile to keep track of hosts that we monitor. An alert is used to trigger if a host is not sending logs for a certain threshold.\nThis search compares the hosts found that are currently sending logs in the configured timewindow and compares them to the alreadt monitored hosts in the lookupfile.\nUniversal forwarders index=_internal sourcetype=splunkd group=tcpin_connections version=* os=* arch=* build=* hostname=* source=*metrics.log | stats latest(version) as version,latest(arch) as arch,latest(os) as os,latest(build) as build by hostname List universal forwarders and their version\n| tstats values(host) where index=* by index Overview of hosts sending logs by index\nindex=_internal source=*metrics.log group=tcpin_connections | stats sum(kb) as total_kb by host, hostname | table hostname, host, total_kb List which hosts are sending through which server/forwarder + volume of data\nIndexes + sourcetypes index=_internal source=*license_usage.log type=\u0026#34;Usage\u0026#34; idx=* | timechart span=1d sum(b) as b by st | foreach * [ eval \u0026lt;\u0026lt;FIELD\u0026gt;\u0026gt; = round(\u0026#39;\u0026lt;\u0026lt;FIELD\u0026gt;\u0026gt;\u0026#39;/1024/1024/1024,2) ] Overview of volume per day by sourcetype\nindex=_internal source=*license_usage.log type=\u0026#34;Usage\u0026#34; earliest=-7d@d latest=@d | eval _time=strftime(_time, \u0026#34;%Y-%m-%d\u0026#34;) | stats sum(b) as bytes by _time | eval GB=round(bytes/1024/1024/1024,2) | table _time GB | sort - _time total volume per day for past 7 days\nindex=_internal sourcetype=splunkd source=*license_usage.log type=Usage earliest=-7d@d latest=@d | stats sum(b) as total_bytes by date_mday, date_month, date_year | eval total_gb = total_bytes / (1024*1024*1024) | stats avg(total_gb) as average_gb_per_day Check average of GB per day over past 7 days\nindex=_internal source=*license_usage.log type=\u0026#34;Usage\u0026#34; | bin _time span=1d | stats sum(b) as b by idx, _time | eval GB=round((b/31556952),2) | eventstats sum(GB) as total_GB by _time | eval percentage=round((GB/total_GB)*100,2) | sort -_time -GB | fields - b, total_GB | rename idx as Index, GB as \u0026#34;Size (GB)\u0026#34;, percentage as \u0026#34;Percentage (%)\u0026#34; | convert timeformat=\u0026#34;%Y-%m-%d\u0026#34; ctime(_time) as Date | fields Date, Index, \u0026#34;Size (GB)\u0026#34;, \u0026#34;Percentage (%)\u0026#34; ```for years, divide b/31556952; for days, divide b/31556952``` Usage per day by index + percentages\n| rest splunk_server=* /services/data/indexes | eval \u0026#34;Retention Period (months)\u0026#34;=round((frozenTimePeriodInSecs/2628000),0) | search NOT title IN (\u0026#34;_*\u0026#34;, \u0026#34;main\u0026#34;, \u0026#34;history\u0026#34;, \u0026#34;summary\u0026#34;, \u0026#34;splunklogger\u0026#34;) | table title \u0026#34;Retention Period (months)\u0026#34; | rename title as Index ```for years, divide frozenTimePeriodInSecs/31556952; for days, divide frozenTimePeriodInSecs/86400``` List indexes with the retention period\n| tstats values(sourcetype) as sourcetype WHERE index=* OR index=_* by index List sourcetypes per index\n| tstats values(source) as source WHERE index=* NOT index=_* by index, host List sources per host, per index\n| metadata type=sourcetypes index=\u0026lt;index_name\u0026gt; | table sourcetype List sourcetypes in specific index\nindex=_internal source=*license_usage.log type=\u0026#34;Usage\u0026#34; idx=\u0026lt;index_name\u0026gt; | stats sum(b) as b by st | eval GB=round((b/1024/1024/1024),2) | eventstats sum(GB) as total_GB | eval percentage=round((GB/total_GB)*100,2) | sort -GB | fields - b, total_GB | rename st as \u0026#34;Sourcetype\u0026#34;, GB as \u0026#34;Size (GB)\u0026#34;, percentage as \u0026#34;Percentage (%)\u0026#34; | fields \u0026#34;Sourcetype\u0026#34;, \u0026#34;Size (GB)\u0026#34;, \u0026#34;Percentage (%)\u0026#34; List sourcetypes in specific index by volume per sourcetype\nindex=eu_cato | dedup sourcetype | table _time sourcetype _raw Quick and dirty example log extraction per sourcetype\nDatamodels | tstats count FROM datamodel=\u0026lt;datamode_name\u0026gt; BY sourcetype find sourcetypes used in specific datamodel\n| rest /servicesNS/-/-/saved/searches splunk_server=local | rex field=title \u0026#34;^(standard|custom)\\-production\\-alert\\-(?\u0026lt;SPA\u0026gt;.*)\u0026#34; | where SPA!=\u0026#34;\u0026#34; | rex field=qualifiedSearch \u0026#34;collect\\sindex=(?\u0026lt;into_index\u0026gt;(sla|slad))\\s.*\u0026#34; | eval into_index = coalesce(into_index, \u0026#39;action.logevent.param.index\u0026#39;) | fillnull value=\u0026#34;-\u0026#34; into_index | search into_index=* eai:acl.app=\u0026lt;app_name\u0026gt; | eval is_scheduled=if(is_scheduled==1 AND disabled==0,\u0026#34;Active\u0026#34;,\u0026#34;Not active\u0026#34;) | rex field=description \u0026#34;(?\u0026lt;use_case\u0026gt;^([^.]+))\u0026#34; | rex field=search \u0026#34;(FROM datamodel=(\\\u0026#34;)?(?\u0026lt;tstats_datamodel\u0026gt;\\w+))\u0026#34; | rex field=search \u0026#34;(WHERE nodename=\\\u0026#34;(\\w+\\.)?(?\u0026lt;tstats_nodename\u0026gt;\\w+))\u0026#34; | rex field=search \u0026#34;(?\u0026lt;first_row\u0026gt;[^\\r\\n].*)\u0026#34; | rex field=first_row \u0026#34;(\\`(?\u0026lt;macro\u0026gt;.*?)\\`)|(\\| from datamodel (?\u0026lt;datamodel\u0026gt;.*))|(index=\\\u0026#34;?(?\u0026lt;index\u0026gt;\\w+)\\\u0026#34;?)|(sourcetype=\\\u0026#34;?(?\u0026lt;sourcetype\u0026gt;\\w+(:\\w+)?)\\\u0026#34;?)\u0026#34; | eval dependency_tmp = case(isnotnull(tstats_datamodel) AND isnotnull(tstats_nodename), tstats_datamodel + \u0026#34;.\u0026#34; + tstats_nodename, isnotnull(tstats_datamodel) AND isnull(tstats_nodename), tstats_datamodel, isnotnull(macro), \u0026#34;`\u0026#34; + macro + \u0026#34;`\u0026#34;) | eval dependency = coalesce(dependency_tmp, datamodel, index, sourcetype) | eval indicator_type = case(isnotnull(tstats_datamodel) OR isnotnull(tstats_nodename), \u0026#34;tstats\u0026#34;, isnotnull(macro), \u0026#34;macro\u0026#34;, isnotnull(datamodel), \u0026#34;datamodel\u0026#34;, isnotnull(index), \u0026#34;index\u0026#34;, isnotnull(sourcetype), \u0026#34;sourcetype\u0026#34;, true(), \u0026#34;other\u0026#34;) | table title, SPA, use_case, indicator_type, dependency, is_scheduled, into_index, cron_schedule, | rename use_case AS \u0026#34;use case\u0026#34; | sort + SPA | fields - SPA | search dependency IN (\u0026lt;data_model1\u0026gt;, \u0026lt;data_model2\u0026gt;) Find alerts related to datamodels\nMind that this is something custom and might not work in your environment. This is more of a sidenote for me to reference to.\nLicenses | rest splunk_server=local \u0026#34;/services/licenser/licenses\u0026#34; | eval creation_time=strftime(creation_time,\u0026#34;%d-%m-%Y\u0026#34;), days_until_expiration=round((expiration_time-now())/86400) , expiration=strftime(expiration_time,\u0026#34;%d-%m-%Y\u0026#34;) ,quota = (\u0026#39;quota\u0026#39;/1024/1024/1024), Volume = quota+\u0026#34; GB\u0026#34;, is_unlimited =if(\u0026#39;is_unlimited\u0026#39;==0,\u0026#34;no\u0026#34;,\u0026#34;yes\u0026#34;) | fields creation_time days_until_expiration expiration expiration_time Volume stack_id status subgroup_id type window_period max_violations label is_unlimited group_id features eai:acl.perms.write eai:acl.perms.read quota | eval renewal_period = case(days_until_expiration\u0026lt;14, \u0026#34;critical\u0026#34;, days_until_expiration\u0026lt;30, \u0026#34;warning\u0026#34;, 1=1, \u0026#34;Ok\u0026#34;) | eval quota1 = case(quota\u0026gt;1000, quota/1024) , quota1 = quota1+\u0026#34; TB\u0026#34; | eval quota2 = case(quota\u0026lt;1, quota*1024) , quota2 = quota2+\u0026#34; MB\u0026#34; | eval Volume = coalesce(quota1,quota2,Volume) | fields group_id label Volume creation_time expiration renewal_period days_until_expiration is_unlimited | rename creation_time AS \u0026#34;Purchase\u0026#34;, expiration AS \u0026#34;Expiration\u0026#34;, renewal_period AS \u0026#34;Renewal\u0026#34;, days_until_expiration AS \u0026#34;Remaining Days\u0026#34;, is_unlimited AS Unlimited, group_id AS \u0026#34;License Type\u0026#34; , features AS \u0026#34;Available Features\u0026#34; , label AS License List information about Licenses, including expiration dates.\nDeployment server + apps | rest /services/deployment/server/applications | stats list(title), count(title) by serverclasses | rename list(title) as apps, count(title) as \u0026#34;amount of apps\u0026#34; List serverclasses and apps\n| rest /services/apps/local | search disabled IN (\u0026#34;false\u0026#34;,0) | table title version description splunk_server List all apps\nParsing + knowledge objects | rest /services/data/transforms/extractions | table eai:acl.app, title, SOURCE_KEY, REGEX, FORMAT, DEST_KEY | sort eai:acl.app title | eval DEST_KEY=if(DEST_KEY=\u0026#34;\u0026#34;,\u0026#34;N/A\u0026#34;,DEST_KEY) | rename eai:acl.app as App, title as Title, SOURCE_KEY as \u0026#34;Source Key\u0026#34;, REGEX as RegEx, FORMAT as Format, DEST_KEY as \u0026#34;Dest Key\u0026#34; List all extractions\n| rest /servicesNS/-/-/admin/directory count=0 splunk_server=local | rename eai:* as *, acl.* as * | eval updated=strptime(updated,\u0026#34;%Y-%m-%dT%H:%M:%S%Z\u0026#34;), updated=if(isnull(updated),\u0026#34;Never\u0026#34;,strftime(updated,\u0026#34;%d %b %Y\u0026#34;)) | sort type | stats list(title) as title, list(type) as type, list(orphaned) as orphaned, list(sharing) as sharing, list(owner) as owner, list(updated) as updated by app List all knowledge objects\nMaintenance: Using the following query, it is often used after a Splunk version or app/addon upgrade to compare the amount of logs.\nKeep in mind that it is using stats, so big pools of data might slow the search down.\nindex=* earliest=-1mon@m-15m latest=-1mon@m | stats min(_time) as _time count as Count by sourcetype, index | eval Day=\u0026#34;Last month\u0026#34; | fields Day, _time, Count, sourcetype, index | append [ search index=* earliest=-1w@m-15m latest=-1w@m | stats min(_time) as _time count as Count by sourcetype, index | eval Day=\u0026#34;Last week\u0026#34; | fields Day, _time, Count, sourcetype, index] | append [ search index=* earliest=-15m@m latest=@m | stats min(_time) as _time count as Count by sourcetype, index | eval Day=\u0026#34;Today\u0026#34; | fields Day, _time, Count, sourcetype, index] | sort sourcetype Compare amount of events per sourcetype last 15 minutes with same 15 minutes last week and last month.\n","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://siemforge.xyz/notes/2.splunk/spl/","summary":"\u003cdiv style=\"display: block; width: 100%; max-width: none;\"\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cblockquote\u003e\n\u003cp\u003eMost queries are tested on my onprem homelab servers. Sometimes rights might be limited for example in SplunkCloud for \u0026ldquo;| rest\u0026rdquo; queries.\u003cbr\u003e\nUsing tstats is faster to search in large datapools.\u003c/p\u003e\n\u003c/blockquote\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003c!-- Troubleshooting:  --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eTroubleshooting:\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cp\u003eWhen troubleshooting I mostly start with this query:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eindex\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e_internal log_level\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eERROR source\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;/opt/splunk/var/log/splunk/splunkd.log\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eErrorcheck Splunkd.log within Splunk GUI\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eindex\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;index_name\u0026#34;\u003c/span\u003e host\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;host_name\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e| sort sourcetype, _time \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e| streamstats current\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003ef last\u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003e_time\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e as prev_time by sourcetype \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e| eval gap\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e_time - prev_time \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e| where gap \u0026gt; \u003cspan style=\"color:#ae81ff\"\u003e3600\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e| eval gap_in_hours\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eround\u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003egap/3600,2\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e| eval prev_time_str\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003estrftime\u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003eprev_time, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e  \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e| table  sourcetype _time prev_time prev_time_str gap gap_in_hours\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCheck for gaps in log ingestion and return results over 1 hour gaps.\nTroubleshooting for false positive due to unreliable log ingestion of a logtype while other logtypes indicated ongoing log ingestion.\u003c/p\u003e","tags":null,"title":"Splunk searches"},{"categories":null,"contents":" Debug refresh: If you change layout, configuration, or app files in Splunk, you can reload the resources through the web GUI, which is faster than restarting Splunk. If you still don’t see your changes, you might need to restart Splunk completely to clear cached data and make sure all updates are applied.\nhttps://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/debug/refresh JavaScript (JS) files: Updates to scripts that run in the browser UI. XML and HTML files: Changes to the layout, structure, or look of the interface. Configuration files: Settings that control how Splunk works. App files: Other resources and assets within your Splunk apps that have been changed. resource\nSSO bypass local account login I’ve seen several setups where SSO is turned on but not set up correctly, which can lead to users having the wrong permissions. Even if local accounts are still enabled, the login URL will redirect to authenticate with SAML or another service. To log in with a local account instead, just add this to the end of the original URL: /en-US/account/login?loginType=splunk\nhttps://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/en-US/account/login?loginType=splunk resource\n","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://siemforge.xyz/notes/2.splunk/gui/","summary":"\u003cdiv style=\"display: block; width: 100%; max-width: none;\"\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eDebug refresh:\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cp\u003eIf you change layout, configuration, or app files in Splunk, you can reload the resources through the web GUI, which is faster than restarting Splunk.\nIf you still don’t see your changes, you might need to restart Splunk completely to clear cached data and make sure all updates are applied.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ehttps://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/debug/refresh\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eJavaScript (JS) files:\u003c/strong\u003e Updates to scripts that run in the browser UI.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eXML and HTML files:\u003c/strong\u003e Changes to the layout, structure, or look of the interface.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConfiguration files:\u003c/strong\u003e Settings that control how Splunk works.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eApp files:\u003c/strong\u003e Other resources and assets within your Splunk apps that have been changed.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://dev.splunk.com/enterprise/docs/developapps/manageknowledge/assetcaching/?_gl=1*16bjotw*_gcl_aw*R0NMLjE3NTk5ODg2ODQuQ2owS0NRandsNWpIQmhESEFSSXNBQjBZcWp3NHJhQk1uUUVyeEc5OXcweGt0TU0tTWl6cjB2N05lV1RiYi1kSGt4endkOHZWQ0NFVy1YUWFBa2l3RUFMd193Y0I.*_gcl_au*NDc3MjU1Nzc0LjE3NTg2MDcyNjY.*FPAU*NDc3MjU1Nzc0LjE3NTg2MDcyNjY.*_ga*Mzg3OTA2MjM0LjE3NDI0NjQ3MzU.*_ga_5EPM2P39FV*czE3NjExMTYwNjgkbzkzJGcwJHQxNzYxMTE2MDY4JGo2MCRsMCRoMTIyNDM2OTU4MQ..*_fplc*RXhXckVEOVBuVjZxRzQ0M3RhJTJGWGVmVXJVdEFkRHFOUlZFdHAlMkZXbzRreHZBSEpDMVclMkIzN2JtMkJwMUZMTGpoamtEOGo5NjFGQnhhalZBSGpzNDdZSHZqY2syRjc0WFJXWTFISUNyeTcxOEtTWnZEZEQxeER0QyUyRjhzUndLUGclM0QlM0Q.\" target=\"_blank\" rel=\"noopener\"\u003eresource\u003c/a\u003e\u003c/p\u003e","tags":null,"title":"Splunk searches"},{"categories":null,"contents":" CLI commands: These are some commands to use in CLI, while a gui is available, I like to do most things using the command line interface.\nBasic operations /opt/splunk/bin/splunk start Start Splunk\n/opt/splunk/bin/splunk stop Stop Splunk\n/opt/splunk/bin/splunk restart Restart Splunk\n/opt/splunk/bin/splunk status Check if Splunk is running\nGeneral checks /opt/splunk/bin/splunk list user | grep admin -B2 Check Splunk admins\nSplunk service setup /opt/splunk/bin/splunk enable Enable Splunk service to start when the host boots up\n/opt/splunk/bin/splunk disable Disable Splunk service so it doesn\u0026rsquo;t start when the host boots up\nDeployment server /opt/splunk/bin/splunk reload deploy-server -class [serverclass-name] Reload a serverclass to push deployment apps without restarting Splunk\u0026quot;\nLicenses /opt/splunk/bin/splunk list licenses | grep \u0026#34;expiration_time\u0026#34; | awk -F\u0026#39;:\u0026#39; \u0026#39;{print $2}\u0026#39; | xargs -I{} date -d @{} +\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34; Check license expiration date\nApps/addons /opt/splunk/bin/splunk list app List installed apps and their status\n/opt/splunk/bin/splunk list app | grep version /opt/splunk/etc/apps/*/default/app.conf List installed apps and their version (if found)\n/splunk install app \u0026lt;path to app.package\u0026gt; Install an app\n/splunk install app \u0026lt;path to app.package\u0026gt; -update 1 Update an app\n/opt/splunk/bin/splunk remove app [appname] Remove an app\nExtra cat /opt/splunk/var/log/splunk/splunkd_stdout.log | grep \u0026#34;Splunk\u0026gt;\u0026#34; | tail -n 1 Find the startup message\n","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://siemforge.xyz/notes/2.splunk/cli/","summary":"\u003cdiv style=\"display: block; width: 100%; max-width: none;\"\u003e\n\u003c!-- CLI commands: --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eCLI commands:\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cp\u003eThese are some commands to use in CLI, while a gui is available, I like to do most things using the command line interface.\u003c/p\u003e\n\u003ch3 id=\"basic-operations\"\u003eBasic operations\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e/opt/splunk/bin/splunk start\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStart Splunk\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e/opt/splunk/bin/splunk stop\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStop Splunk\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e/opt/splunk/bin/splunk restart\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRestart Splunk\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e/opt/splunk/bin/splunk status\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCheck if Splunk is running\u003c/p\u003e\n\u003ch3 id=\"general-checks\"\u003eGeneral checks\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e/opt/splunk/bin/splunk list user | grep admin -B2\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCheck Splunk admins\u003c/p\u003e\n\u003ch3 id=\"splunk-service-setup\"\u003eSplunk service setup\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e/opt/splunk/bin/splunk enable\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eEnable Splunk service to start when the host boots up\u003c/p\u003e","tags":null,"title":"Splunk CLI commands"},{"categories":null,"contents":" Upgrade procedure: Splunk Upgrade Guide This document provides a step-by-step procedure to upgrade Splunk from an existing version to a newer release. Follow each section carefully to ensure a smooth upgrade process.\nTable of Contents Preparation Download and Transfer Files Pre-Upgrade Checks Device State Management Backup Current Configuration Install the New Version Post-Upgrade Verification Documentation and Communication 1. Preparation Ensure you have access to the target devices and necessary permissions. Confirm the current Splunk version. Notify customer and colleagues about the upcoming upgrade. Review release notes for the target version. 2. Download and Transfer Files Download the Splunk Installer You can use the wget command to download the package on the host wget -O splunk---Linux-x86_64.tgz \u0026ldquo;https://download.splunk.com/products/splunk/releases//linux/splunk---Linux-x86_64.tgz\u0026rdquo;\nsudo -i cd /tmp wget -O splunk-9.4.3-237ebbd22314-linux-amd64.tgz \u0026#34;https://download.splunk.com/products/splunk/releases/9.4.3/linux/splunk-9.4.3-237ebbd22314-linux-amd64.tgz\u0026#34; Transfer the Installer to Target Devices Use WinSCP or similar tools to connect and send to the device: put \u0026ldquo;path\\to\\splunk---Linux-x86_64.tgz\u0026rdquo; \u0026ldquo;/tmp/splunk---Linux-x86_64.tgz\u0026rdquo;\nset path=C:\\Program Files (x86)\\WinSCP;%path% winscp sftp://\u0026lt;CLI-account\u0026gt;@\u0026lt;device_NAT_IP\u0026gt;:/tmp put \u0026#34;path\\to\\splunk-9.4.3-237ebbd22314-linux-amd64.tgz\u0026#34; \u0026#34;/tmp/splunk-9.4.3-237ebbd22314-linux-amd64.tgz\u0026#34; Use SCP on more recent windows machines or on Linux:\nscp \u0026#34;path\\to\\splunk-9.4.3-237ebbd22314-linux-amd64.tgz\u0026#34; user@remote-ip:\u0026#34;/tmp/splunk-9.4.3-237ebbd22314-linux-amd64.tgz\u0026#34; 3. Pre-Upgrade Checks Check if Splunk has an active process:\nps -ef | grep splunk Check if Splunk is running:\n/opt/splunk/bin/splunk status Check Splunk version:\n/opt/splunk/bin/splunk --version 4. Device State Management Check whether a monitoring needs to set the host in maintenance mode\nOn Splunk in environments with index clusters: Maintenance mode halts most bucket fixup activity and prevents frequent rolling of hot buckets. It is useful when performing peer upgrades and other maintenance activities on an indexer cluster. Enable maintenance mode:\n/opt/splunk/bin/splunk enable maintenance-mode Disable maintenance mode:\n/opt/splunk/bin/splunk disable maintenance-mode Enable maintenance mode:\n/opt/splunk/bin/splunk show maintenance-mode 5. Backup Current Configuration Check KV Store status\n/opt/splunk/bin/splunk show kvstore-status Backup KV Store\n/opt/splunk/bin/splunk backup kvstore Check backup\nls -la /opt/splunk/var/lib/splunk/kvstorebackup/ Backup config files\nsudo -i cd /tmp mkdir /tmp/etc_backup$(date +\u0026#34;%d-%m-%Y\u0026#34;) cp -a /opt/splunk/etc/ /tmp/etc_backup$(date +\u0026#34;%d-%m-%Y\u0026#34;)/ Check backup\nls /tmp/etc_backup$(date +\u0026#34;%d-%m-%Y\u0026#34;)/etc 6. Install the New Version stop splunk using Splunk command\n/opt/splunk/bin/splunk stop stop splunk using systemctl command\nsystemctl stop splunk Unpack the Installer\ntar -xvzf /tmp/splunk-9.4.3-237ebbd22314-linux-amd64.tgz -C /opt/ Change the owner of the Splunk folder recursively with the account running Splunk\nchown -R splunk.splunk /opt/splunk Since I\u0026rsquo;m using a splunk user account (created without elevated privileges) to run splunk, I am switching to the account\nsu - splunk Start Splunk with License Acceptance\n/opt/splunk/bin/splunk start --accept-license --answer-yes 7. Post-Upgrade Verification Check if Splunk is running\n/opt/splunk/bin/splunk status Check recent logs for errors\ntail -20f /opt/splunk/var/log/splunk/splunkd.log Check Splunk version\n/opt/splunk/bin/splunk --version Check if Splunk service is run by splunk user\nps -ef | grep splunkd | grep -v grep 8. Documentation and Communication Update internal documentation with the new version details. Notify customer and colleagues of the successful upgrade. Share release notes and any post-upgrade instructions. ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://siemforge.xyz/notes/2.splunk/splunk-upgrade/","summary":"\u003cdiv style=\"display: block; width: 100%; max-width: none;\"\u003e\n\u003c!-- Upgrade procedure: --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eUpgrade procedure:\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003ch1 id=\"splunk-upgrade-guide\"\u003eSplunk Upgrade Guide\u003c/h1\u003e\n\u003cp\u003eThis document provides a step-by-step procedure to upgrade Splunk from an existing version to a newer release. Follow each section carefully to ensure a smooth upgrade process.\u003c/p\u003e\n\u003ch2 id=\"table-of-contents\"\u003eTable of Contents\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003ePreparation\u003c/li\u003e\n\u003cli\u003eDownload and Transfer Files\u003c/li\u003e\n\u003cli\u003ePre-Upgrade Checks\u003c/li\u003e\n\u003cli\u003eDevice State Management\u003c/li\u003e\n\u003cli\u003eBackup Current Configuration\u003c/li\u003e\n\u003cli\u003eInstall the New Version\u003c/li\u003e\n\u003cli\u003ePost-Upgrade Verification\u003c/li\u003e\n\u003cli\u003eDocumentation and Communication\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"1-preparation\"\u003e1. Preparation\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eEnsure you have access to the target devices and necessary permissions.\u003c/li\u003e\n\u003cli\u003eConfirm the current Splunk version.\u003c/li\u003e\n\u003cli\u003eNotify customer and colleagues about the upcoming upgrade.\u003c/li\u003e\n\u003cli\u003eReview release notes for the target version.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"2-download-and-transfer-files\"\u003e2. Download and Transfer Files\u003c/h2\u003e\n\u003ch3 id=\"download-the-splunk-installer\"\u003eDownload the Splunk Installer\u003c/h3\u003e\n\u003cp\u003eYou can use the wget command to download the package on the host\nwget -O splunk-\u003cversion\u003e-\u003chash\u003e-Linux-x86_64.tgz \u0026ldquo;\u003ca href=\"https://download.splunk.com/products/splunk/releases/\" target=\"_blank\" rel=\"noopener\"\u003ehttps://download.splunk.com/products/splunk/releases/\u003c/a\u003e\u003cversion\u003e/linux/splunk-\u003cversion\u003e-\u003chash\u003e-Linux-x86_64.tgz\u0026rdquo;\u003c/p\u003e","tags":null,"title":"Splunk Upgrade procedure"},{"categories":null,"contents":" Troubleshooting: Troubleshooting A collection of KQL I use to troubleshoot incidents in Sentinel.\nFailed analytics rule SentinelHealth | where SentinelResourceName startswith \u0026#34;AnalyticsRule -\u0026#34; | where OperationName == \u0026#34;Scheduled analytics rule run\u0026#34; | where Status == \u0026#34;Failure\u0026#34; | summarize count(Status) by SentinelResourceName | where count_Status \u0026gt; 3 | extend info_name = \u0026#34;failed_analytic-rule\u0026#34; | extend info_sub_route = \u0026#34;Sentinel\u0026#34; Detects rules that have failed more than three times, indicating potential issues with rule execution.\nSentinelHealth | where SentinelResourceName in (\u0026#34;Successful logon from IP and failure from a different IP\u0026#34;) | where Status == \u0026#34;Failure\u0026#34; | project TimeGenerated, SentinelResourceName, Status | sort by TimeGenerated desc Finding the failed runs of an analytics rule can help narrow down the time window to investigate further. SentinelHealth\nSelects data from the SentinelHealth table.\n| where SentinelResourceName in (\u0026ldquo;Successful logon from IP and failure from a different IP\u0026rdquo;)\nFilters records to include only those with specific resource names related to logon success and failure.\n| where Status == \u0026ldquo;Failure\u0026rdquo;\nFurther filters the data to include only failed login attempts.\n| project TimeGenerated, SentinelResourceName, Status\nSelects only the columns for the time of the event, resource name, and status.\n| sort by TimeGenerated desc\nOrders the results by the most recent events first.\nSentinelHealth | where SentinelResourceName in (\u0026#34;Successful logon from IP and failure from a different IP\u0026#34;) | project TimeGenerated, SentinelResourceName, Status | sort by TimeGenerated desc Expand the failed run time window to investigate warnings or successful runs. It is possible that Sentinel runs out of resources where a failed run can be expected and closed as true benign positive, expected behavior. Multiple failed runs might indicate another issue with the analytics rule (Syntax related or other) after an update of the rule. Further investigation is needed to resolve the issue.\nTable does not receive data union withsource=TableName vcenter_CL, SecurityEvent, CommonSecurityLog, Syslog // Add/remove tables as needed | where TimeGenerated \u0026gt;= ago(7d) | summarize MinutesSinceLastEvent = datetime_diff(\u0026#34;minute\u0026#34;, now(), max(TimeGenerated)), LastIngestionTime = max(TimeGenerated) by TableName | where MinutesSinceLastEvent \u0026gt;= 60 | project TableName, alert_minutes_last_event = MinutesSinceLastEvent, LastIngestionTime | extend info_name = \u0026#34;table_not_receiving-data\u0026#34; | extend info_sub_route = \u0026#34;Sentinel\u0026#34; Identifies tables that haven\u0026rsquo;t received data for over an hour, signaling possible ingestion problems.\nSecurityAlert | summarize EventCount = count() by bin(TimeGenerated, 1h) | render timechart This query counts the amount of events per hour and renders it in a timechart. This is an easy way to pinpoint anomalies in log event ingestion.\nSecurityAlert | where TimeGenerated \u0026gt;= ago(30d) | sort by TimeGenerated asc | serialize | extend TimeDiff = datetime_diff(\u0026#39;second\u0026#39;, TimeGenerated, prev(TimeGenerated)) | where TimeDiff \u0026gt;= 3600 | project TimeGenerated, prev_TimeGenerated=prev(TimeGenerated), TimeDiff Statistical overview of gaps (over 1 hour) between events for the last 30 days. This one is mostly used to determine tuning for the threshold used in an analytic rule to check if a table did not receive data. (could indicate a logsource stopped sending or a misconfiguration in sending the logs to another table)\nSilent logsource let _SecurityEvent = SecurityEvent | where TimeGenerated \u0026gt; ago(15d) | summarize MinutesSinceLastEvent = datetime_diff(\u0026#34;minute\u0026#34;, now(), max(TimeGenerated)), LastIngestionTime = max(TimeGenerated) by Asset = Computer, Table = Type, sourcetype = EventSourceName ; let _Syslog = Syslog | where TimeGenerated \u0026gt; ago(15d) | summarize MinutesSinceLastEvent = datetime_diff(\u0026#34;minute\u0026#34;, now(), max(TimeGenerated)), LastIngestionTime = max(TimeGenerated) by Asset = Computer, Table = Type //sourcetype = strcat( // iff(DeviceVendor == \u0026#34;Fortinet\u0026#34;, DeviceEventCategory, \u0026#34;\u0026#34;), // iff(DeviceVendor == \u0026#34;Palo Alto Networks\u0026#34;, Activity, \u0026#34;\u0026#34;), // iff(DeviceVendor == \u0026#34;F5\u0026#34;, DeviceProduct, \u0026#34;\u0026#34;) // ) ; let _Heartbeat = Heartbeat | where TimeGenerated \u0026gt; ago(15d) | summarize MinutesSinceLastEvent = datetime_diff(\u0026#34;minute\u0026#34;, now(), max(TimeGenerated)), LastIngestionTime = max(TimeGenerated) by Asset = Computer, Table = Type, sourcetype = Type ; union _SecurityEvent, _Syslog, _Heartbeat | join _GetWatchlist(\u0026#39;CriticalLogs\u0026#39;) on $left.Asset == $right.Host, $left.sourcetype == $right.SubType, $left.Table == $right.DataTable | where Maintenance != \u0026#34;TRUE\u0026#34; | where MinutesSinceLastEvent \u0026gt; toint(Threshold) * 60 // Watchlist in hours need to convert to min | project Table, Asset, MinutesSinceLastEvent, sourcetype, SubType, Threshold, LastIngestionTime | extend info_name = \u0026#34;critical_log_source_goes-silent\u0026#34; | extend info_sub_route = \u0026#34;Sentinel\u0026#34; Finds critical logs from assets that haven\u0026rsquo;t reported in over a specified threshold, suggesting potential issues or outages.\nSecurityEvent | where Computer contains (\u0026#34;test-pc\u0026#34;) | summarize Type = count() by Computer, bin(TimeGenerated, 1h) | render timechart This KQL query filters security events to include only those from computers with \u0026ldquo;test-pc\u0026rdquo; in their name, then counts the number of events for each computer grouped by 1-hour intervals, and finally displays the results as a time chart. I use this to check for patterns in log ingestion which can indicate ingestion issues or expected behavior.\nReporting: Reporting _GetWatchlist(\u0026#39;CriticalLogs\u0026#39;) | project Host, Vendor = SubType, Threshold, Table = DataTable, Updated = LastUpdatedTimeUTC Example query to display the content of a watchlist in Azure Sentinel. Columns are renamed in output using following format \u0026ldquo; = \u0026rdquo;, project operator only returns provided columns.\nUsage | where IsBillable == true | where QuantityUnit == \u0026#34;MBytes\u0026#34; | summarize totalMB = sum(Quantity) by day = bin(TimeGenerated, 1d) | summarize avgGBperDay = round(avg(totalMB / 1024), 2) A Sentinel KQL query to extract the average daily volume, adjust time range as needed.\nUsage | where IsBillable == true | where QuantityUnit == \u0026#34;MBytes\u0026#34; | summarize totalMB = sum(Quantity) by day = bin(TimeGenerated, 1d) | extend GB = round(totalMB / 1024, 2) | order by day asc A Sentinel KQL query aggregated the total number of bytes ingested per day.\n","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://siemforge.xyz/notes/3.sentinel/kql/","summary":"\u003cdiv style=\"display: block; width: 100%; max-width: none;\"\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eTroubleshooting:\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003ch1 id=\"troubleshooting\"\u003eTroubleshooting\u003c/h1\u003e\n\u003cp\u003eA collection of KQL I use to troubleshoot incidents in Sentinel.\u003c/p\u003e\n\u003ch2 id=\"failed-analytics-rule\"\u003eFailed analytics rule\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eSentinelHealth\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e| where SentinelResourceName startswith \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;AnalyticsRule -\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e| where OperationName \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Scheduled analytics rule run\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e| where Status \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Failure\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e| summarize count\u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003eStatus\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e by SentinelResourceName\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e| where count_Status \u0026gt; \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e| extend info_name \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;failed_analytic-rule\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e| extend info_sub_route \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Sentinel\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eDetects rules that have failed more than three times, indicating potential issues with rule execution.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eSentinelHealth\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e| where SentinelResourceName in \u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Successful logon from IP and failure from a different IP\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e| where Status \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Failure\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e| project TimeGenerated, SentinelResourceName, Status\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e| sort by TimeGenerated desc\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eFinding the failed runs of an analytics rule can help narrow down the time window to investigate further.\n\u003cstrong\u003eSentinelHealth\u003c/strong\u003e\u003cbr\u003e\nSelects data from the SentinelHealth table.\u003cbr\u003e\n\u003cstrong\u003e| where SentinelResourceName in (\u0026ldquo;Successful logon from IP and failure from a different IP\u0026rdquo;)\u003c/strong\u003e\u003cbr\u003e\nFilters records to include only those with specific resource names related to logon success and failure.\u003cbr\u003e\n\u003cstrong\u003e| where Status == \u0026ldquo;Failure\u0026rdquo;\u003c/strong\u003e\u003cbr\u003e\nFurther filters the data to include only failed login attempts.\u003cbr\u003e\n\u003cstrong\u003e| project TimeGenerated, SentinelResourceName, Status\u003c/strong\u003e\u003cbr\u003e\nSelects only the columns for the time of the event, resource name, and status.\u003cbr\u003e\n\u003cstrong\u003e| sort by TimeGenerated desc\u003c/strong\u003e\u003cbr\u003e\nOrders the results by the most recent events first.\u003c/p\u003e","tags":null,"title":"Kusto query language"},{"categories":null,"contents":" Logstash management I found these notes from a colleague who worked on a Logstash machine.\nalias logtail=\u0026#39;tail -f /var/log/logstash/logstash-plain.log\u0026#39; alias logconf=\u0026#39;cd /etc/logstash/conf.d\u0026#39; alias logstart=\u0026#39;systemctl start logstash\u0026#39; alias logstop=\u0026#39;systemctl stop logstash\u0026#39; alias logrestart=\u0026#39;systemctl restart logstash\u0026#39; alias logstatus=\u0026#39;systemctl status logstash\u0026#39; alias logsamp=\u0026#39;cd /usr/share/logstash/samples\u0026#39; logtail: Continuously displays new entries in the Logstash log file.\nlogconf: Navigates to the Logstash configuration directory.\nlogstart: Starts the Logstash service.\nlogstop: Stops the Logstash service.\nlogrestart: Restarts the Logstash service.\nlogstatus: Checks the current status of the Logstash service.\nlogsamp: Navigates to the directory containing sample Logstash configurations.\nMaking alias permanent for all users To ensure these aliases are available for all users permanently, you can add them to a global shell configuration file such as /etc/bashrc, /etc/bash.bashrc, or /etc/profile, depending on your system and shell preferences.\nsudo nano /etc/bashrc Edit bashrc file.\nalias logtail=\u0026#39;tail -f /var/log/logstash/logstash-plain.log\u0026#39; alias logconf=\u0026#39;cd /etc/logstash/conf.d\u0026#39; alias logstart=\u0026#39;systemctl start logstash\u0026#39; alias logstop=\u0026#39;systemctl stop logstash\u0026#39; alias logrestart=\u0026#39;systemctl restart logstash\u0026#39; alias logstatus=\u0026#39;systemctl status logstash\u0026#39; alias logsamp=\u0026#39;cd /usr/share/logstash/samples\u0026#39; add the alias commands at the end of the file.\nsource /etc/bashrc Save the file and reload the shell configuration.\n","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://siemforge.xyz/notes/3.sentinel/logstash_cli/","summary":"\u003cdiv style=\"display: block; width: 100%; max-width: none;\"\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003ch1 id=\"logstash-management\"\u003eLogstash management\u003c/h1\u003e\n\u003cp\u003eI found these notes from a colleague who worked on a Logstash machine.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ealias logtail\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;tail -f /var/log/logstash/logstash-plain.log\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ealias logconf\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;cd /etc/logstash/conf.d\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ealias logstart\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;systemctl start logstash\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ealias logstop\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;systemctl stop logstash\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ealias logrestart\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;systemctl restart logstash\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ealias logstatus\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;systemctl status logstash\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ealias logsamp\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;cd /usr/share/logstash/samples\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003elogtail:\u003c/strong\u003e Continuously displays new entries in the Logstash log file.\u003cbr\u003e\n\u003cstrong\u003elogconf:\u003c/strong\u003e Navigates to the Logstash configuration directory.\u003cbr\u003e\n\u003cstrong\u003elogstart:\u003c/strong\u003e Starts the Logstash service.\u003cbr\u003e\n\u003cstrong\u003elogstop:\u003c/strong\u003e Stops the Logstash service.\u003cbr\u003e\n\u003cstrong\u003elogrestart:\u003c/strong\u003e Restarts the Logstash service.\u003cbr\u003e\n\u003cstrong\u003elogstatus:\u003c/strong\u003e Checks the current status of the Logstash service.\u003cbr\u003e\n\u003cstrong\u003elogsamp:\u003c/strong\u003e Navigates to the directory containing sample Logstash configurations.\u003c/p\u003e","tags":null,"title":"Logstash CLI commands"},{"categories":null,"contents":" Troubleshooting (files): Find a file or directory\nfind / -type f -name \u0026#34;filename\u0026#34; 2\u0026gt;/dev/null find / -type d -name \u0026#34;dirname\u0026#34; 2\u0026gt;/dev/null find files that contain a pattern\nfind . -type f -exec grep -l \u0026#39;version\u0026#39; {} \\; Troubleshooting (system): Check logs if a service has executed/failed before (I needed to check if logrotate was running daily, also checked for rsyslog after)\njournalctl | grep logrotate check if machine is a vm or barebone\ndmidecode -s system-manufacturer check folder disk usage\ndu -hs * | sort -h check open port\n(echo \u0026gt; /dev/tcp/10.254.4.54/22) \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026amp; echo \u0026#34;It\u0026#39;s up\u0026#34; || echo \u0026#34;It\u0026#39;s down\u0026#34; check for listening ports\nsudo lsof -i -P -n | grep LISTEN sudo netstat -tulpn | grep LISTEN sudo ss -tulpn | grep LISTEN sudo lsof -i:22 ## see a specific port such as 22 ## sudo nmap -sTU -O IP-address-Here Troubleshooting (SSL): Check expiry date on .pem file\nopenssl x509 -enddate -noout -in /path/to/certificate.pem Maintenance: dry-run an update on OS\nsudo yum check-update Check if a reboot is required\nneeds-restarting -r remove cache of updates for old data\nrm -rf /var/cache/yum block icmp\nsysctl -w net.ipv4.icmp_echo_ignore_all=1 File manipulation: remove empty lines from file\nsed -i \u0026#39;/^$/d\u0026#39; \u0026lt;filename\u0026gt; Disk manipulation: Check logs while plugging in the new drive\ndmesg --follow List all block devices, including filesystems, UUIDs and labels\nlsblk -o NAME,SIZE,TYPE,FSTYPE,LABEL,UUID,MOUNTPOINT detailed info about new disk (Replace X with your new disk e.g. \u0026ndash;name=/dev/sdb)\nudevadm info --query=all --name=/dev/sdX Filesystem details (if you encounter command not found -\u0026gt; try sudo)\nblkid SMART/health info (Replace X with your new disk e.g. \u0026ndash;name=/dev/sdb)\nsudo smartctl -a -d scsi /dev/sdX Account control: change to root\nsudo -i Grep sudo users\nrm -f /tmp/names; for user in $(getent passwd | cut -d: -f1); do count=$((count+1)); if sudo -l -U \u0026#34;$user\u0026#34; | grep -q \u0026#34;ALL\u0026#34;; then echo \u0026#34;$user\u0026#34; \u0026gt;\u0026gt; /tmp/names; echo \u0026#34;Checked $count of $(getent passwd | cut -d: -f1 | wc -l) users.\u0026#34;; fi; done; clear; cat /tmp/names; rm -f /tmp/names ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://siemforge.xyz/notes/4.linux/cli/","summary":"\u003cdiv style=\"display: block; width: 100%; max-width: none;\"\u003e\n\u003c!-- Troubleshooting: --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eTroubleshooting (files):\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cp\u003eFind a file or directory\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003efind / -type f -name \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;filename\u0026#34;\u003c/span\u003e 2\u0026gt;/dev/null\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003efind / -type d -name \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;dirname\u0026#34;\u003c/span\u003e 2\u0026gt;/dev/null\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003efind files that contain a pattern\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003efind . -type f -exec grep -l \u003cspan style=\"color:#e6db74\"\u003e\u0026#39;version\u0026#39;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e{}\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e\\;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003c!-- Troubleshooting: --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eTroubleshooting (system):\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cp\u003eCheck logs if a service has executed/failed before (I needed to check if logrotate was running daily, also checked for rsyslog after)\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ejournalctl | grep logrotate\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003echeck if machine is a vm or barebone\u003c/p\u003e","tags":null,"title":"Linux Bash commands"},{"categories":null,"contents":" Logrotate Configuration Cheat Sheet: This cheat sheet provides an extensive list of Logrotate configuration directives, their descriptions, and examples.\nUse this as a quick reference to master log rotation on Unix-like systems.\nBasic structure: Each configuration block is tied to a log file or set of log files. Example:\n/var/log/example.log { daily rotate 7 compress missingok notifempty create 0640 root adm } Configuration Directives \u0026amp; Examples: Basic Settings: rotate \u0026lt;count\u0026gt;\nKeep number of old log files before deleting them.\ndaily | weekly | monthly | yearly\nFrequency of rotation.\nCompression Options: compress\nCompress old versions of log files with gzip.\nnocompress\nDo not compress old logs.\ndelaycompress\nPostpone compression to the next rotation cycle (used with compress).\nFile Handling: missingok\nIgnore missing log files and don’t issue an error.\nnotifempty\nDo not rotate the log if it is empty.\nifempty\nRotate the log even if it is empty (default behavior).\ncreate \u0026lt;mode\u0026gt; \u0026lt;owner\u0026gt; \u0026lt;group\u0026gt;\nCreate a new log file with specified permissions.\ncopy\nMake a copy of the log file and truncate the original.\ncopytruncate\nTruncate the original log file after copying it (useful for active logs).\nDate \u0026amp; Naming: dateext\nAppend an extension with the current date to rotated log files.\ndateformat .%Y-%m-%d\nCustom format for dateext (e.g., .2025-06-06).\nextension \u0026lt;ext\u0026gt;\nForce specific extension for rotated files (e.g., .log).\nSize-Based Rotation: maxage \u0026lt;days\u0026gt;\nRemove rotated logs older than .\nminsize \u0026lt;size\u0026gt;\nRotate only if log size is above .\nsize \u0026lt;size\u0026gt;\nRotate if log file size meets threshold, regardless of time.\nmaxsize \u0026lt;size\u0026gt;\nDo not rotate if log is larger than specified size.\nDirectory \u0026amp; Scripts: olddir \u0026lt;dir\u0026gt;\nMove rotated logs to a specified directory.\nsharedscripts\nRun postrotate script once for all matching logs.\npostrotate/endscript\nScript to run after log rotation.\nprerotate/endscript\nScript to run before log rotation.\nfirstaction/endscript\nRun only once before rotation begins (before prerotate).\nlastaction/endscript\nRun once after rotation finishes (after postrotate).\ntabooext + \u0026lt;ext\u0026gt;\nTreat additional extensions as taboo (not rotated).\nFull Example Configuration: /var/log/myapp/*.log { daily rotate 10 size 100M compress delaycompress missingok notifempty create 0640 appuser adm sharedscripts postrotate systemctl reload myapp \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 || true endscript } Tips: Run logrotate -d \u0026lt;config\u0026gt; to debug your config without applying changes. Use logrotate -f \u0026lt;config\u0026gt; to force rotation for testing. Logrotate is typically triggered via cron or systemd timers. Keep your config DRY by centralizing shared logic in /etc/logrotate.conf and using includes. ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://siemforge.xyz/notes/4.linux/logrotate/","summary":"\u003cdiv style=\"display: block; width: 100%; max-width: none;\"\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eLogrotate Configuration Cheat Sheet:\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cblockquote\u003e\n\u003cp\u003eThis cheat sheet provides an extensive list of Logrotate configuration directives, their descriptions, and examples.\u003cbr\u003e\nUse this as a quick reference to master log rotation on Unix-like systems.\u003c/p\u003e\n\u003c/blockquote\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eBasic structure:\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cp\u003eEach configuration block is tied to a log file or set of log files. Example:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e/var/log/example.log \u003cspan style=\"color:#f92672\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    daily\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    rotate \u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    compress\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    missingok\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    notifempty\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    create \u003cspan style=\"color:#ae81ff\"\u003e0640\u003c/span\u003e root adm\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003cp\u003e\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eConfiguration Directives \u0026amp; Examples:\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003ch4 id=\"basic-settings\"\u003eBasic Settings:\u003c/h4\u003e\n\u003cp\u003e\u003ccode\u003erotate \u0026lt;count\u0026gt;\u003c/code\u003e\u003cbr\u003e\nKeep \u003ccount\u003e number of old log files before deleting them.\u003c/p\u003e","tags":null,"title":"Logrotate config"},{"categories":null,"contents":" LVM cheatsheet Acronyms you must know PV = Physical Volume VG = Volume Group LV = Logical Volume PV1 PV2 PV3 PV4 \\ | | / VG1 VG2 | | \\ LV1 LV2 LV3 Step-by-Step LVM Setup 1. Add a Physical Disk connect a physical/virtual disk to your system.\nOverview of block devices\nlsblk Used/available space on mounted filesystems\ndf -h 2. Create Physical Volumes Info\npvck # Check PV metadata pvdisplay # Display PV attributes pvs # Report PV information pvscan # Scan for PVs Create\npvcreate /dev/sda /dev/sdb /dev/sdc /dev/sdd Delete\npvremove /dev/sdX Edit\npvchange # Change PV attributes pvmove # Move physical extents pvresize # Resize PV 3. Create Volume Groups Info\nvgck # Check VG metadata vgdisplay # VG attributes vgs # Report VG info vgscan # Scan for VGs Create\nvgcreate vg0 /dev/sda Delete\nvgremove vg_name Edit\nvgcfgbackup # Backs up volume group (VG) metadata vgcfgrestore # Restores VG metadata from backup vgchange # Change VG attributes vgconvert # Metadata format change vgexport # Exports a VG to make it unknown to the system vgimport # Imports a VG previously exported vgimportclone # Imports and renames a VG to avoid conflicts vgmerge # Merges two VGs into one vgsplit # Splits a VG into two separate VGs vgextend vg0 /dev/sdc /dev/sdd # Adds physical volumes to a VG vgreduce # Removes physical volumes from a VG vgrename # Renames a VG vgmknodes # Recreates device nodes for LVM devices 4. Create Logical Volumes Info\nlvdisplay # Show LV attributes lvmdiskscan # Scan for devices lvs # Report info lvscan # Scan for LVs Create\nlvcreate -n lvbackup -L 50G vgbackup -r Delete\nlvremove /dev/vg_name/lv_name Edit\nlvchange # Change LV attributes lvconvert # Mirror/snapshot conversion lvextend /dev/vg_name/lv_name lvreduce # Reduce size lvresize # Resize lvrename # Rename resize2fs /dev/vg0/lv_root # Resize filesystem xfs_growfs -d /dev/vg-group-name/lv-name 5. Create a filesystem on your Logical Volume mkfs.ext4 /dev/vgbackup/lvbackup 6. Mounting the Logical Volume blkid /dev/vgbackup/lvbackup # Get UUID mkdir /path/to/folder # Mount point vim /etc/fstab # Add entry with UUID mount -a # Mount all from fstab Overview Commands pvs # List physical volumes vgs # List volume groups lvs # List logical volumes pvdisplay vgdisplay lvdisplay Snapshotting lvcreate --size 1G --snapshot --name snap_name /dev/vg0/lvdata lvconvert --merge /dev/vg0/snap_name Filesystem Resize After LV Resize lvextend -L +10G /dev/vgbackup/lvbackup resize2fs /dev/vgbackup/lvbackup xfs_growfs /path/to/your/mount/point Helpful Tips Always back up metadata: vgcfgbackup Reload partition tables without rebooting: partprobe /dev/sdX ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://siemforge.xyz/notes/4.linux/lvm/lvm_cheatsheet/","summary":"\u003cdiv style=\"display: block; width: 100%; max-width: none;\"\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eLVM cheatsheet\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003ch2 id=\"acronyms-you-must-know\"\u003eAcronyms you must know\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePV\u003c/strong\u003e = Physical Volume\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVG\u003c/strong\u003e = Volume Group\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLV\u003c/strong\u003e = Logical Volume\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e    PV1  PV2   PV3  PV4\n      \\  |      |  /\n       VG1      VG2\n        |       |  \\ \n       LV1     LV2  LV3\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003ch2 id=\"step-by-step-lvm-setup\"\u003eStep-by-Step LVM Setup\u003c/h2\u003e\n\u003ch3 id=\"1-add-a-physical-disk\"\u003e1. Add a Physical Disk\u003c/h3\u003e\n\u003cp\u003econnect a physical/virtual disk to your system.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eOverview of block devices\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003elsblk\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003eUsed/available space on mounted filesystems\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edf -h\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003ch3 id=\"2-create-physical-volumes\"\u003e2. Create Physical Volumes\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eInfo\u003c/strong\u003e\u003c/p\u003e","tags":null,"title":"LVM cheatsheet"},{"categories":null,"contents":" fdisk cheatsheet: fdisk is a powerful, text-based utility used to create, delete, and manage disk partitions in Linux systems. It supports MBR (Master Boot Record) partition tables and is best suited for systems not using GPT (GUID Partition Table).\nBasic syntax fdisk [options] /dev/sdX Where /dev/sdX is the disk you want to operate on (e.g., /dev/sda, /dev/sdb).\nAlways double-check the disk name to avoid data loss.\nKey commands in fdisk interactive mode Command Description m Print help menu p Display existing partition table n Create a new partition d Delete a partition t Change a partition\u0026rsquo;s system ID (type) a Toggle bootable flag w Write changes and exit q Quit without saving changes Common workflow examples 1. View partition table\nsudo fdisk -l Lists all disks and their partitions.\n2. Start fdisk on a specific disk\nsudo fdisk /dev/sdX Enters interactive mode.\n3. Create a new partition\nCommand (m for help): n Select default or choose primary (p) or extended (e) Partition number: 1 First sector: [Press Enter to accept default] Last sector: +1G # or specify size like +20G 4. Change partition type\nCommand (m for help): t Partition number: 1 Hex code (type L for list): 83 # Linux filesystem 5. Set bootable flag\nCommand (m for help): a Partition number: 1 6. Write changes to disk\nCommand (m for help): w Writes the partition table to disk and exits.\n7. Quit without saving\nCommand (m for help): q Exits without modifying the disk.\nImportant options -f # Use a script file -l # List partition tables for all devices -u # Display sectors in cylinders or sectors Partition type codes Code Filesystem Type 83 Linux 82 Linux swap 7 HPFS/NTFS/exFAT b W95 FAT32 c W95 FAT32 (LBA) a5 FreeBSD To list all type codes:\nCommand (m for help): L Post partitioning After creating partitions, always format them:\nmkfs.ext4 /dev/sdX1 mkfs.vfat /dev/sdX2 Creating a label for the disk:\ne2label /dev/sdX1 nvmeExternal Mount the partition:\nmount /dev/sdX1 /mnt/mydisk Configure in fstab:\nLABEL=nvmeExternal /mnt/sdX1 ext4 defaults,nofail,x-systemd.device-timeout=10 0 2 Mount all disks configured in fstab\nmount -a Helpful tips Use partprobe or reboot after changing partition table: sudo partprobe /dev/sdX Use lsblk or blkid to view and identify new partitions: lsblk blkid ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://siemforge.xyz/notes/4.linux/fdisk/","summary":"\u003cdiv style=\"display: block; width: 100%; max-width: none;\"\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003efdisk cheatsheet:\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cp\u003efdisk is a powerful, text-based utility used to create, delete, and manage disk partitions in Linux systems. It supports MBR (Master Boot Record) partition tables and is best suited for systems not using GPT (GUID Partition Table).\u003c/p\u003e\n\u003ch2 id=\"basic-syntax\"\u003eBasic syntax\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003efdisk \u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003eoptions\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e /dev/sdX\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eWhere \u003ccode\u003e/dev/sdX\u003c/code\u003e is the disk you want to operate on (e.g., \u003ccode\u003e/dev/sda\u003c/code\u003e, \u003ccode\u003e/dev/sdb\u003c/code\u003e).\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAlways double-check the disk name to avoid data loss.\u003c/p\u003e","tags":null,"title":"fdisk"},{"categories":null,"contents":" TCPDump cheatsheet General Syntax tcpdump [options] [expression]\nPacket Capturing Options Switch Syntax Description -i any tcpdump -i any Capture from all interfaces -i eth0 tcpdump -i eth0 Capture from specific interface ( Ex Eth0) -c tcpdump -i eth0 -c 10 Capture first 10 packets and exit -D tcpdump -D Show available interfaces -A tcpdump -i eth0 -A Print in ASCII -w tcpdump -i eth0 -w tcpdump.txt To save capture to a file -r tcpdump -r tcpdump.txt Read and analyze saved capture file -n tcpdump -n -I eth0 Do not resolve host names -nn tcpdump -n -i eth0 Stop Domain name translation and lookups (Host names or port names ) tcp tcpdump -i eth0 -c 10 -w tcpdump.pcap tcp Capture TCP packets only port tcpdump -i eth0 port 80 Capture traffic from a defined port only host tcpdump host 192.168.1.100 Capture packets from specific host net tcpdump net 10.1.1.0/16 Capture files from network subnet src tcpdump src 10.1.1.100 Capture from a specific source address dst tcpdump dst 10.1.1.100 Capture from a specific destination address \u0026lt;service\u0026gt; tcpdump http Filter traffic based on a port number for a service \u0026lt;port\u0026gt; tcpdump port 80 Filter traffic based on a service port range tcpdump portrange 21-125 Filter based on port range -S tcpdump -S http Display entire packet ipv6 tcpdunp -IPV6 Show only IPV6 packets -d tcpdump -d tcpdump.pcap display human readable form in standard output -F tcpdump -F tcpdump.pcap Use the given file as input for filter -I tcpdump -I eth0 set interface as monitor mode -L tcpdump -L Display data link types for the interface -N tcpdump -N tcpdump.pcap not printing domian names -K tcpdump -K tcpdump.pcap Do not verify checksum -p tcpdump -p -i eth0 Not capturing in promiscuous mode Logical Operators Operator Syntax Example Description AND and, \u0026amp;\u0026amp; tcpdump -n src 192.168.1.1 and dst port 21 Combine filtering options OR or, EXCEPT not, ! tcpdump dst 10.1.1.1 and not icmp Negation of the condition LESS \u0026lt; tcpdump \u0026lt;32 Shows packets size less than 32 GREATER \u0026gt; tcpdump \u0026gt;=32 Shows packets size greater than 32 Output options Switch Description -q Quite and less verbose mode display less details -t Do not print time stamp details in dump -v Little verbose output -vv More verbose output -vvv Most verbose output -x Print data and headers in HEX format -xx Print data with link headers in HEX format -X Print output in HEX and ASCII format excluding link headers -XX Print output in HEX and ASCII format including link headers -e Print Link (Ethernet) headers -S Print sequence numbers in exact format Protocols Ether fddi icmp ip ip6 ppp radio rarp slip tcp udp wlan Common Commands with Protocols for Filtering Captures Command Description src/ dsthost (host name or IP) Filter by source or destination IP address or host ether src/ dst host (ethernet host name or IP) Ethernet host filtering by source or destination src/ dstnet (subnet mask in CIDR) Filter by subnet tcp/udp src/dst port ( port number) Filter TCP or UDP packets by source or destination port tcp/udp src/dst port range ( port number range) Filter TCP or UDP packets by source or destination port range ether/ip broadcast Filter for Ethernet or IP broadcasts ether/ip multicast Filter for Ethernet or IP multicasts ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://siemforge.xyz/notes/4.linux/tcpdump/tcpdump_cheatsheet/","summary":"\u003cdiv style=\"display: block; width: 100%; max-width: none;\"\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eTCPDump cheatsheet\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003ch1 id=\"general-syntax\"\u003eGeneral Syntax\u003c/h1\u003e\n\u003cp\u003etcpdump [options] [expression]\u003c/p\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003ch1 id=\"packet-capturing-options\"\u003ePacket Capturing Options\u003c/h1\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eSwitch\u003c/th\u003e\n          \u003cth\u003eSyntax\u003c/th\u003e\n          \u003cth\u003eDescription\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-i any\u003c/td\u003e\n          \u003ctd\u003etcpdump -i any\u003c/td\u003e\n          \u003ctd\u003eCapture from all interfaces\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-i eth0\u003c/td\u003e\n          \u003ctd\u003etcpdump -i eth0\u003c/td\u003e\n          \u003ctd\u003eCapture from specific interface ( Ex Eth0)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-c\u003c/td\u003e\n          \u003ctd\u003etcpdump -i eth0 -c 10\u003c/td\u003e\n          \u003ctd\u003eCapture first 10 packets and exit\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-D\u003c/td\u003e\n          \u003ctd\u003etcpdump -D\u003c/td\u003e\n          \u003ctd\u003eShow available interfaces\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-A\u003c/td\u003e\n          \u003ctd\u003etcpdump -i eth0 -A\u003c/td\u003e\n          \u003ctd\u003ePrint in ASCII\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-w\u003c/td\u003e\n          \u003ctd\u003etcpdump -i eth0 -w tcpdump.txt\u003c/td\u003e\n          \u003ctd\u003eTo save capture to a file\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-r\u003c/td\u003e\n          \u003ctd\u003etcpdump -r tcpdump.txt\u003c/td\u003e\n          \u003ctd\u003eRead and analyze saved capture file\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-n\u003c/td\u003e\n          \u003ctd\u003etcpdump -n -I eth0\u003c/td\u003e\n          \u003ctd\u003eDo not resolve host names\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-nn\u003c/td\u003e\n          \u003ctd\u003etcpdump -n -i eth0\u003c/td\u003e\n          \u003ctd\u003eStop Domain name translation and lookups (Host names or port names )\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003etcp\u003c/td\u003e\n          \u003ctd\u003etcpdump -i eth0 -c 10 -w tcpdump.pcap tcp\u003c/td\u003e\n          \u003ctd\u003eCapture TCP packets only\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eport\u003c/td\u003e\n          \u003ctd\u003etcpdump -i eth0 port 80\u003c/td\u003e\n          \u003ctd\u003eCapture traffic from a defined port only\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ehost\u003c/td\u003e\n          \u003ctd\u003etcpdump host 192.168.1.100\u003c/td\u003e\n          \u003ctd\u003eCapture packets from specific host\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003enet\u003c/td\u003e\n          \u003ctd\u003etcpdump net 10.1.1.0/16\u003c/td\u003e\n          \u003ctd\u003eCapture files from network subnet\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003esrc\u003c/td\u003e\n          \u003ctd\u003etcpdump src 10.1.1.100\u003c/td\u003e\n          \u003ctd\u003eCapture from a specific source address\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003edst\u003c/td\u003e\n          \u003ctd\u003etcpdump dst 10.1.1.100\u003c/td\u003e\n          \u003ctd\u003eCapture from a specific destination address\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003e\u0026lt;service\u0026gt;\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003etcpdump http\u003c/td\u003e\n          \u003ctd\u003eFilter traffic based on a port number for a service\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003e\u0026lt;port\u0026gt;\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003etcpdump port 80\u003c/td\u003e\n          \u003ctd\u003eFilter traffic based on a service\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eport range\u003c/td\u003e\n          \u003ctd\u003etcpdump portrange 21-125\u003c/td\u003e\n          \u003ctd\u003eFilter based on port range\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-S\u003c/td\u003e\n          \u003ctd\u003etcpdump -S http\u003c/td\u003e\n          \u003ctd\u003eDisplay entire packet\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eipv6\u003c/td\u003e\n          \u003ctd\u003etcpdunp -IPV6\u003c/td\u003e\n          \u003ctd\u003eShow only IPV6 packets\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-d\u003c/td\u003e\n          \u003ctd\u003etcpdump -d tcpdump.pcap\u003c/td\u003e\n          \u003ctd\u003edisplay human readable form in standard output\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-F\u003c/td\u003e\n          \u003ctd\u003etcpdump -F tcpdump.pcap\u003c/td\u003e\n          \u003ctd\u003eUse the given file as input for filter\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-I\u003c/td\u003e\n          \u003ctd\u003etcpdump -I eth0\u003c/td\u003e\n          \u003ctd\u003eset interface as monitor mode\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-L\u003c/td\u003e\n          \u003ctd\u003etcpdump -L\u003c/td\u003e\n          \u003ctd\u003eDisplay data link types for the interface\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-N\u003c/td\u003e\n          \u003ctd\u003etcpdump -N tcpdump.pcap\u003c/td\u003e\n          \u003ctd\u003enot printing domian names\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-K\u003c/td\u003e\n          \u003ctd\u003etcpdump -K tcpdump.pcap\u003c/td\u003e\n          \u003ctd\u003eDo not verify checksum\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-p\u003c/td\u003e\n          \u003ctd\u003etcpdump -p -i eth0\u003c/td\u003e\n          \u003ctd\u003eNot capturing in promiscuous mode\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch1 id=\"logical-operators\"\u003eLogical Operators\u003c/h1\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eOperator\u003c/th\u003e\n          \u003cth\u003eSyntax\u003c/th\u003e\n          \u003cth\u003eExample\u003c/th\u003e\n          \u003cth\u003eDescription\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eAND\u003c/td\u003e\n          \u003ctd\u003eand, \u0026amp;\u0026amp;\u003c/td\u003e\n          \u003ctd\u003etcpdump -n src 192.168.1.1 and dst port 21\u003c/td\u003e\n          \u003ctd\u003eCombine filtering options\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eOR\u003c/td\u003e\n          \u003ctd\u003eor,\u003c/td\u003e\n          \u003ctd\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eEXCEPT\u003c/td\u003e\n          \u003ctd\u003enot, !\u003c/td\u003e\n          \u003ctd\u003etcpdump dst 10.1.1.1 and not icmp\u003c/td\u003e\n          \u003ctd\u003eNegation of the condition\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eLESS\u003c/td\u003e\n          \u003ctd\u003e\u0026lt;\u003c/td\u003e\n          \u003ctd\u003etcpdump \u0026lt;32\u003c/td\u003e\n          \u003ctd\u003eShows packets size less than 32\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eGREATER\u003c/td\u003e\n          \u003ctd\u003e\u0026gt;\u003c/td\u003e\n          \u003ctd\u003etcpdump \u0026gt;=32\u003c/td\u003e\n          \u003ctd\u003eShows packets size greater than 32\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch1 id=\"output-options\"\u003eOutput options\u003c/h1\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eSwitch\u003c/th\u003e\n          \u003cth\u003eDescription\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-q\u003c/td\u003e\n          \u003ctd\u003eQuite and less verbose mode display less details\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-t\u003c/td\u003e\n          \u003ctd\u003eDo not print time stamp details in dump\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-v\u003c/td\u003e\n          \u003ctd\u003eLittle verbose output\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-vv\u003c/td\u003e\n          \u003ctd\u003eMore verbose output\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-vvv\u003c/td\u003e\n          \u003ctd\u003eMost verbose output\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-x\u003c/td\u003e\n          \u003ctd\u003ePrint data and headers in HEX format\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-xx\u003c/td\u003e\n          \u003ctd\u003ePrint data with link headers in HEX format\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-X\u003c/td\u003e\n          \u003ctd\u003ePrint output in HEX and ASCII format excluding link headers\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-XX\u003c/td\u003e\n          \u003ctd\u003ePrint output in HEX and ASCII format including link headers\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-e\u003c/td\u003e\n          \u003ctd\u003ePrint Link (Ethernet) headers\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e-S\u003c/td\u003e\n          \u003ctd\u003ePrint sequence numbers in exact format\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch1 id=\"protocols\"\u003eProtocols\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eEther\u003c/li\u003e\n\u003cli\u003efddi\u003c/li\u003e\n\u003cli\u003eicmp\u003c/li\u003e\n\u003cli\u003eip\u003c/li\u003e\n\u003cli\u003eip6\u003c/li\u003e\n\u003cli\u003eppp\u003c/li\u003e\n\u003cli\u003eradio\u003c/li\u003e\n\u003cli\u003erarp\u003c/li\u003e\n\u003cli\u003eslip\u003c/li\u003e\n\u003cli\u003etcp\u003c/li\u003e\n\u003cli\u003eudp\u003c/li\u003e\n\u003cli\u003ewlan\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"common-commands-with-protocols-for-filtering-captures\"\u003eCommon Commands with Protocols for Filtering Captures\u003c/h1\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eCommand\u003c/th\u003e\n          \u003cth\u003eDescription\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003esrc/ dsthost (host name or IP)\u003c/td\u003e\n          \u003ctd\u003eFilter by source or destination IP address or host\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eether src/ dst host (ethernet host name or IP)\u003c/td\u003e\n          \u003ctd\u003eEthernet host filtering by source or destination\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003esrc/ dstnet (subnet mask in CIDR)\u003c/td\u003e\n          \u003ctd\u003eFilter by subnet\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003etcp/udp src/dst port ( port number)\u003c/td\u003e\n          \u003ctd\u003eFilter TCP or UDP packets by source or destination port\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003etcp/udp src/dst port range ( port number range)\u003c/td\u003e\n          \u003ctd\u003eFilter TCP or UDP packets by source or destination port range\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eether/ip broadcast\u003c/td\u003e\n          \u003ctd\u003eFilter for Ethernet or IP broadcasts\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eether/ip multicast\u003c/td\u003e\n          \u003ctd\u003eFilter for Ethernet or IP multicasts\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e","tags":null,"title":"TCPDump cheatsheet"},{"categories":null,"contents":" Script: add sudo user Create a sudo user that won\u0026rsquo;t prompt for password on executing sudo commands.\n#!/bin/bash # Ensure script is run as root if [[ $EUID -ne 0 ]]; then echo \u0026#34;❌ This script must be run as root\u0026#34; exit 1 fi # Prompt for new username read -p \u0026#34;Enter new username: \u0026#34; username # Check if user already exists if id \u0026#34;$username\u0026#34; \u0026amp;\u0026gt;/dev/null; then echo \u0026#34;⚠️ User \u0026#39;$username\u0026#39; already exists.\u0026#34; exit 1 fi # Prompt for password (silent input) read -s -p \u0026#34;Enter password for $username: \u0026#34; password echo read -s -p \u0026#34;Confirm password: \u0026#34; password_confirm echo # Check passwords match if [[ \u0026#34;$password\u0026#34; != \u0026#34;$password_confirm\u0026#34; ]]; then echo \u0026#34;❌ Passwords do not match.\u0026#34; exit 1 fi # Create user with home directory and bash shell useradd -m -s /bin/bash \u0026#34;$username\u0026#34; # Set user password echo \u0026#34;${username}:${password}\u0026#34; | chpasswd # Add user to sudo group usermod -aG sudo \u0026#34;$username\u0026#34; # Create a sudoers file to allow passwordless sudo echo \u0026#34;$username ALL=(ALL) NOPASSWD:ALL\u0026#34; \u0026gt; \u0026#34;/etc/sudoers.d/$username\u0026#34; chmod 440 \u0026#34;/etc/sudoers.d/$username\u0026#34; echo \u0026#34;✅ User \u0026#39;$username\u0026#39; created with bash shell and passwordless sudo access.\u0026#34; ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://siemforge.xyz/notes/4.linux/scripts/addsudouser/","summary":"\u003cdiv style=\"display: block; width: 100%; max-width: none;\"\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eScript: add sudo user\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cp\u003eCreate a sudo user that won\u0026rsquo;t prompt for password on executing sudo commands.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#!/bin/bash\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Ensure script is run as root\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e[[\u003c/span\u003e $EUID -ne \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e]]\u003c/span\u003e; \u003cspan style=\"color:#66d9ef\"\u003ethen\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e   echo \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;❌ This script must be run as root\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e   exit \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efi\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Prompt for new username\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eread -p \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Enter new username: \u0026#34;\u003c/span\u003e username\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Check if user already exists\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e id \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e$username\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e \u0026amp;\u0026gt;/dev/null; \u003cspan style=\"color:#66d9ef\"\u003ethen\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    echo \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;⚠️ User \u0026#39;\u003c/span\u003e$username\u003cspan style=\"color:#e6db74\"\u003e\u0026#39; already exists.\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    exit \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efi\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Prompt for password (silent input)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eread -s -p \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Enter password for \u003c/span\u003e$username\u003cspan style=\"color:#e6db74\"\u003e: \u0026#34;\u003c/span\u003e password\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eecho\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eread -s -p \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Confirm password: \u0026#34;\u003c/span\u003e password_confirm\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eecho\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Check passwords match\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e[[\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e$password\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e !\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e$password_confirm\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e]]\u003c/span\u003e; \u003cspan style=\"color:#66d9ef\"\u003ethen\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    echo \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;❌ Passwords do not match.\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    exit \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efi\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Create user with home directory and bash shell\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003euseradd -m -s /bin/bash \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e$username\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Set user password\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eecho \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e${\u003c/span\u003eusername\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e${\u003c/span\u003epassword\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e | chpasswd\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Add user to sudo group\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eusermod -aG sudo \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e$username\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Create a sudoers file to allow passwordless sudo\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eecho \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e$username\u003cspan style=\"color:#e6db74\"\u003e ALL=(ALL) NOPASSWD:ALL\u0026#34;\u003c/span\u003e \u0026gt; \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;/etc/sudoers.d/\u003c/span\u003e$username\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003echmod \u003cspan style=\"color:#ae81ff\"\u003e440\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;/etc/sudoers.d/\u003c/span\u003e$username\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eecho \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;✅ User \u0026#39;\u003c/span\u003e$username\u003cspan style=\"color:#e6db74\"\u003e\u0026#39; created with bash shell and passwordless sudo access.\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003c/div\u003e","tags":null,"title":"scriptSudoUser"},{"categories":null,"contents":" ncdu: Ncdu is a disk usage analyzer with a text-mode user interface. It is designed to find space hogs on a remote server where you don’t have an entire graphical setup available, but it is a useful tool even on regular desktop systems. Ncdu aims to be fast, simple, easy to use, and should be able to run on any POSIX-like system.\ninstall:\nwget https://dev.yorhel.nl/download/ncdu-2.8.1-linux-x86_64.tar.gz tar -xzf ncdu-2.8.1-linux-x86_64.tar.gz sudo mv ncdu /usr/local/bin/ usage:\nncdu \u0026lt;directory_name\u0026gt; ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://siemforge.xyz/notes/5.tools/ncdu/","summary":"\u003cdiv style=\"display: block; width: 100%; max-width: none;\"\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003encdu:\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cp\u003eNcdu is a disk usage analyzer with a text-mode user interface. It is designed to find space hogs on a remote server where you don’t have an entire graphical setup available, but it is a useful tool even on regular desktop systems. Ncdu aims to be fast, simple, easy to use, and should be able to run on any POSIX-like system.\u003cbr\u003e\ninstall:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ewget https://dev.yorhel.nl/download/ncdu-2.8.1-linux-x86_64.tar.gz\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etar -xzf ncdu-2.8.1-linux-x86_64.tar.gz\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esudo mv ncdu /usr/local/bin/\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eusage:\u003c/p\u003e","tags":null,"title":"ncdu"},{"categories":null,"contents":" Python virtual environment: create a virtual environment\npython3 -m venv venv Activate the virtual environment\nsource venv/bin/activate Get out of the virtual environment\ndeactivate ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://siemforge.xyz/notes/6.coding/python/","summary":"\u003cdiv style=\"display: block; width: 100%; max-width: none;\"\u003e\n\u003c!-- Project: --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003ePython virtual environment:\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cp\u003ecreate a virtual environment\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epython3 -m venv venv\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eActivate the virtual environment\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003esource venv/bin/activate\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eGet out of the virtual environment\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edeactivate\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003c/div\u003e","tags":null,"title":"Python"},{"categories":null,"contents":" Version control: What is Version Control? Version control systems are tools that manage changes made to files and directories in a project. They allow you to keep track of what you did when, undo any changes you decide you don\u0026rsquo;t want, and collaborate at scale with others. This cheat sheet focuses on one of the most popular one, Git.\nDefinitions: Basic definition Local repo or repository:\nA local directory containing code and files for the project Remote repository:\nAn online version of the local repository hosted on services like GitHub, GitLab, and BitBucket Cloning:\nThe act of making a clone or copy of a repository in a new directory Commit:\nA snapshot of the project you can come back to Branch:\nA copy of the project used for working in an isolated environment without affecting the main project Git merge:\nThe process of combining two branches together More advanced definition .gitignore file:\nA file that lists other files you want git not to track (e.g. large data folders, private info, and any local files that shouldn’t be seen by the public) Staging area:\na cache that holds changes you want to commit next Git stash:\nanother type of cache that holds unwanted changes you may want to come back later Commit ID or hash:\na unique identifier for each commit, used for switching to different save points HEAD (always capitalized letters):\na reference name for the latest commit, to save you having to type Commit IDs. HEADn syntax is used to refer to older commits (e.g. HEAD2 refers to the second-to-last commit) How to install: On OS X or Windows — Using an installer\nDownload the installer Follow the prompts On OS X — Using Homebrew brew install git On Linux\nsudo apt-get install git Check if installation successful (On any platform)\ngit --version Configuration: If you are working in a team on a single repo, it is important for others to know who made certain changes to the code. So, Git allows you to set user credentials such as name, email, etc..\nbasic information Configure your email git config user.email [your.email@domain.com] Configure your name git config user.name [your-name] Important tags to determine the scope of configurations, Git lets you use tags to determine the scope of the information you’re using during setup\nLocal directory, single project (this is the default tag) git config --local user.email “my_email@example.com” All git projects under the current user git config --global user.email “my_email@example.com” For all users on the current machine git config --system user.email “my_email@example.com” List configurations List all key-value configurations\ngit config --list Get the value of a single key git config --get \u0026lt;key\u0026gt; Setting aliases Create an alias named gc for the “git commit” command git config --global alias.gc commit Example usage:\ngc -m \u0026#34;New commit\u0026#34; Create an alias named ga for the “git add” command git config --global alias.ga add Managing repos: Creating local repositories Clone a repository from remote hosts (GitHub, GitLab, DagsHub, etc.) git clone \u0026lt;remote_repo_url\u0026gt; Initialize git tracking inside the current directory git init Create a git-tracked repository inside a new directory git init [dir_name] Clone only a specific branch git clone -branch \u0026lt;branch_name\u0026gt; \u0026lt;repo_url\u0026gt; Cloning into a specified directory git clone \u0026lt;repo_url\u0026gt; \u0026lt;dir_name\u0026gt; Managing remote repositories List remote repos git remote Create a new connection called to a remote repository on servers like GitHub, GitLab, DagsHub, etc git remote add \u0026lt;remote\u0026gt; \u0026lt;url_to_remote\u0026gt; Remove a connection to a remote repo called git remote rm \u0026lt;remote\u0026gt; Rename a remote connection git remote rename \u0026lt;old_name\u0026gt; \u0026lt;new_name\u0026gt; Handling files: Adding and removing files Add a file or directory to git for tracking git add \u0026lt;filename_or_dir\u0026gt; Add all untracked and tracked files inside the current directory to git git add . Remove a file from a working directory or staging area git rm \u0026lt;filename_or_dir\u0026gt; Saving and working with changes See changes in the local repository git status Saving a snapshot of the staged changes with a custom message git commit -m “[Commit message]” Staging changes in all tracked files and committing with a message git add -am “[Commit message]” Editing the message of the latest commit git commit --amend -m “[New commit message]” A note on stashes Git stash allows you to temporarily save edits you\u0026rsquo;ve made to your working copy so you can return to your work later. Stashing is especially useful when you are not yet ready to commit changes you\u0026rsquo;ve done, but would like to revisit them at a later time.\nSaving staged and unstaged changes to stash for a later use git stash Stashing staged, unstaged and untracked files as well git stash -u Stashing everything (including ignored files) git stash --all Reapply previously stashed changes and empty the stash git stash pop Reapply previously stashed changes and keep the stash git stash apply Dropping changes in the stash git stash drop Show uncommitted changes since the last commit git diff Show the differences between two commits (should provide the commit IDs) git diff \u0026lt;id_1\u0026gt; \u0026lt;id_2\u0026gt; Branches: List all branches git branch git branch --list git branch -a Create a new local branch named new_branch without checking out that branch git branch \u0026lt;new_branch\u0026gt; Switch into an existing branch named git checkout \u0026lt;branch\u0026gt; Create a new local branch and switch into it git checkout -b \u0026lt;new_branch\u0026gt; Safe delete a local branch (prevents deleting unmerged changes) git branch -d \u0026lt;branch\u0026gt; Force delete a local branch (whether merged or unmerged) git branch -D \u0026lt;branch\u0026gt; Rename the current branch to \u0026lt;new_name\u0026gt; git branch -m \u0026lt;new_name\u0026gt; Push a copy of local branch named branch to the remote repo git push \u0026lt;remote_repo\u0026gt; branch~ Delete a remote branch named branch git push \u0026lt;remote_repo\u0026gt; :branch git push \u0026lt;remote_repo\u0026gt; --delete branch Merging a branch into the main branch git checkout main git merge \u0026lt;other_branch\u0026gt; Merging a branch and creating a commit message git merge --no-ff \u0026lt;other_branch\u0026gt; Compare the differences between two branches git diff \u0026lt;branch_1\u0026gt; \u0026lt;branch_2\u0026gt; Compare a single between two branches git diff \u0026lt;branch_1\u0026gt; \u0026lt;branch_2\u0026gt; \u0026lt;file\u0026gt; Handle commits: Pulling changes Download all commits and branches from the without applying them on the local repo git fetch \u0026lt;remote\u0026gt; Only download the specified from the git fetch \u0026lt;remote\u0026gt; \u0026lt;branch\u0026gt; Merge the fetched changes if accepted git merge \u0026lt;remote\u0026gt;/\u0026lt;branch\u0026gt; A more aggressive version of fetch which calls fetch and merge simultaneously git pull \u0026lt;remote\u0026gt; Review work with logs List all commits with their author, commit ID, date and message git log List one commit per line (-n tag can be used to limit the number of commits displayed (e.g. -5)) git log --oneline [-n] Log all commits with diff information: git log --stat Log commits after some date git log --oneline --after=”YYYY-MM-DD\u0026#34; Log commits before some date git log --oneline --before=”last year” Reverse changes Checking out (switching to) older commits git checkout HEAD~2 Checks out the third-to-last commit git checkout \u0026lt;commit_id\u0026gt; Undo the latest commit but leave the working directory unchanged You can undo as many commits as you want by changing the number after the tilde git reset HEAD~1 Discard all changes of the latest commit (no easy recovery) Instead of HEAD~n, you can provide commit hash as well. Changes after that commit will be destroyed git reset --hard HEAD~1 Undo a single given commit, without modifying commits that come after it (a safe reset) git revert [commit_id] sources: DatacampCheatsheet\nGitlabCheatsheet ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://siemforge.xyz/notes/6.coding/git/","summary":"\u003cdiv style=\"display: block; width: 100%; max-width: none;\"\u003e\n\u003c!-- Intro: --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eVersion control:\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003ch1 id=\"what-is-version-control\"\u003eWhat is Version Control?\u003c/h1\u003e\n\u003cp\u003eVersion control systems are tools that manage changes made to files and directories in a project. They allow you to keep track of what you did when, undo any changes you decide you don\u0026rsquo;t want, and collaborate at scale with others. This cheat sheet focuses on one of the most popular one, Git.\u003c/p\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003c!-- Definitions: --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eDefinitions:\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003ch1 id=\"basic-definition\"\u003eBasic definition\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eLocal repo or repository:\u003cbr\u003e\nA local directory containing code and files for the project\u003c/li\u003e\n\u003cli\u003eRemote repository:\u003cbr\u003e\nAn online version of the local repository hosted on services like GitHub, GitLab, and BitBucket\u003c/li\u003e\n\u003cli\u003eCloning:\u003cbr\u003e\nThe act of making a clone or copy of a repository in a new directory\u003c/li\u003e\n\u003cli\u003eCommit:\u003cbr\u003e\nA snapshot of the project you can come back to\u003c/li\u003e\n\u003cli\u003eBranch:\u003cbr\u003e\nA copy of the project used for working in an isolated environment without affecting the main project\u003c/li\u003e\n\u003cli\u003eGit merge:\u003cbr\u003e\nThe process of combining two branches together\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"more-advanced-definition\"\u003eMore advanced definition\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e.gitignore file:\u003cbr\u003e\nA file that lists other files you want git not to track (e.g. large data folders, private info, and any local files that shouldn’t be seen by the public)\u003c/li\u003e\n\u003cli\u003eStaging area:\u003cbr\u003e\na cache that holds changes you want to commit next\u003c/li\u003e\n\u003cli\u003eGit stash:\u003cbr\u003e\nanother type of cache that holds unwanted changes you may want to come back later\u003c/li\u003e\n\u003cli\u003eCommit ID or hash:\u003cbr\u003e\na unique identifier for each commit, used for switching to different save points\u003c/li\u003e\n\u003cli\u003eHEAD (always capitalized letters):\u003cbr\u003e\na reference name for the latest commit, to save you having to type Commit IDs. HEAD\u003cdel\u003en syntax is used to refer to older commits (e.g. HEAD\u003c/del\u003e2 refers to the second-to-last commit)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003c!-- Install: --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eHow to install:\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cp\u003eOn OS X or Windows — Using an installer\u003c/p\u003e","tags":null,"title":"GIT"},{"categories":null,"contents":"Securing the hypervisor before anything else Why hypervisor security comes first In any virtualization environment, the hypervisor represents a high-value target. A compromised virtual machine is an incident. A compromised hypervisor is a catastrophe. Before building labs, automating infrastructure, or experimenting with offensive security techniques, it is essential to establish a hardened Proxmox VE baseline. This article walks through securing a single-node Proxmox installation as the foundation for a security-focused lab.\nUnderstanding Proxmox architecture Proxmox VE is a type-1 hypervisor built on Debian Linux. It combines:\nKVM/QEMU for virtual machines LXC for containers A web-based management UI A REST API Native Linux storage and networking Because everything ultimately runs on Linux, traditional Linux hardening principles apply directly to Proxmox.\nThreat model: Why this matters If an attacker gains control of the Proxmox host, they can:\nAccess every VM disk Snapshot live memory Inject malicious images Disable backups Persist invisibly This makes hypervisor security non-negotiable.\nCreating a non-root administrative user Linux user creation adduser labadmin Proxmox user and RBAC assignment pveum user add labadmin@pam pveum aclmod / -user labadmin@pam -role Administrator Using named accounts instead of root improves accountability and auditability.\nSSH hardening with key-based authentication Generate an SSH Key ssh-keygen -t ed25519 -C \u0026#34;labadmin@proxmox\u0026#34; Install the key ssh-copy-id labadmin@\u0026lt;proxmox-ip\u0026gt; Harden SSH configuration Edit /etc/ssh/sshd_config:\nPermitRootLogin no PasswordAuthentication no PubkeyAuthentication yes Restart SSH: systemctl restart ssh This eliminates password-based attacks and disables remote root access.\nReviewing Proxmox firewall defaults By default, the Proxmox firewall is often disabled. This is not inherently wrong, but it must be a conscious decision. Blindly enabling firewalls without understanding networking can be as dangerous as leaving systems exposed. At this stage, documenting the default state is more important than enforcing rules.\nBackup awareness Before automating backups, administrators should understand:\nWhat data is backed up? Where backups are stored? How restores work? Backup strategy becomes critical once VM templates are introduced.\nDocumentation as a security control Documenting architecture, access patterns, and policies reduces operational risk and improves incident response.\nAt minimum, record:\nHost IP addresses Storage configuration Network bridges Administrative users SSH policies Conclusion Hardening the Proxmox host establishes trust in everything built on top of it. Automation, DevSecOps, and offensive security labs all depend on this foundation.\nIn the next phase, we move inside the virtual machines and begin building secure operating system baselines.\nSecure the hypervisor first — everything else depends on it.\nFAQ 1. Root login disabled remotely, do we still keep root? Yes. Absolutely. Root never goes away. What we disable is remote root login, not root itself. Root must exist, have a strong password stored securely, only be usable via:\nPhysical console Out-of-band management (IPMI) Emergency recovery modes Root is required for: Boot recovery Filesystem repair Broken auth recovery Catastrophic misconfiguration fixes Root over SSH has no attribution, is heavily brute-forced, turns one mistake into total compromise.\n2. What if PermitRootLogin is set to no and I need root locally? PermitRootLogin no only applies to SSH. At the console, logged in as another user with sudo, in single-user mode, you can still become root.\nSSH policy ≠ local access policy.\n3. What if I lose my private key AND password auth is disabled? This is a real incident scenario, not a hypothetical.\nRecovery paths:\nConsole access Physical keyboard + screen Proxmox web console (if still logged in) Out-of-band management IPMI / iDRAC / iLO Single-user mode Rescue ISO It is important that there is documentation that says how to recover. If none of these exist, the system is effectively lost.\n4. What would be an example of documentation? Access Control Documentation Proxmox Host Access Policy Administrative Accounts root (local only, SSH disabled) labadmin (primary admin, SSH key-based) btg-admin (break-glass, disabled by default) SSH Policy PasswordAuthentication: disabled RootLogin: disabled Authentication: ed25519 keys only SSH Port: 22 (subject to future VPN restriction) Recovery Procedures Console login as root IPMI access (credentials stored in password manager) Single-user mode enabled via GRUB Audit Notes All administrative actions performed via named accounts Root usage logged via local console access This is DORA / ISO / SOC2 friendly: clear ownership, clear controls, clear recovery.\n5. What is PAM in Proxmox? PAM: Pluggable Authentication Modules\nPAM = Linux system users (Auth is delegated to /etc/passwd, /etc/shadow, PAM stack)\nOther Proxmox auth realms:\npam → local Linux users\npve → Proxmox-only users\nldap, ad, oidc → enterprise auth\nRealm Meaning labadmin@pam Linux user user@pve Proxmox-only user user@ldap External directory 6. What is pveum? pveum = Proxmox User Manager.\npveum ties Linux identity to Proxmox RBAC.\nAuthentication vs Authorization\nPAM → authentication (who are you?) pveum → authorization (what can you do?) PAM user must exist in Linux, Proxmox must know if the user exists + what permissions they have.\n7. What is aclmod and what are ACLs? ACL = Access Control List\nACLs define who (user/group), what (resource path), how (role / permissions)\nExample:\npveum aclmod / -user labadmin@pam -role Administrator Translation:\nApply to / (entire datacenter) User: labadmin Permissions: Administrator role ACL hierarchy\n/ ├── nodes/ ├── storage/ ├── vms/ 8. Difference between Proxmox admin and root? Root Proxmox Admin Linux superuser Proxmox RBAC role Unrestricted OS access Scoped to Proxmox objects No audit trail Fully auditable Breaks everything Can be limited 9. What is ed25519? Ed25519 is a modern, high-performance public-key signature system based on Elliptic Curve Cryptography (ECC). It is designed for fast signing and verification of signatures, offering significantly smaller keys and higher security levels compared to traditional RSA. It is now considered the standard for modern SSH keys, code signing, and secure communications.\nWhy not skip the passphrase? Without a passphrase, steal the key = instant access.\nMalware reads ~/.ssh/id_ed25519 → done.\nWith a passphrase key theft alone is insufficient, requires user interaction or agent compromise\nThis is defense-in-depth.\nWhy not rotate keys often? Key rotation sounds good, but in practice -\u0026gt; breaks automation, breaks access unexpectedly, adds operational risk\nIndustry practice:\nRotate on compromise Rotate on role change Rotate periodically, but not constantly Passphrases reduce the need for aggressive rotation.\n10. Break-glass accounts a good idea? Yes. But one break-glass admin, disabled by default, password stored offline, logged usage, reviewed periodically. No multiple active BTG accounts, that increases attack surface.\n11. “Easy pivot into every VM”, even without VM creds? Because Proxmox controls VM disks, VM memory, VM consoles\u0026hellip;\nWith Proxmox admin access you can mount disks, reset passwords, inject startup scripts, snapshot memory.\nYou don’t need VM credentials if you own the hypervisor.\n12. What is IPMI? IPMI = Out-of-band management (Power control, console access, BIOS access, local shell = keyboard + monitor)\niDRAC (Dell) - Integrated dell remote access controller iLO (HP) - Integrated lights-out Supermicro IPMI - Intelligent platform management interface Out-of-band (OOB) management means that you can manage the system even if the OS is down. it is completely separate from SSH, network config, firewall rules, Linux itself. It is like a power button, keyboard + monitor, BIOS but over the network.\n13. What is single-user mode? Linux recovery mode (Minimal services, root shell, no networking).\nUsed for password resets, filesystem repair, auth recovery.\n14. Is console access last resort with root? Yes and that’s by design. Remote convenience is sacrificed for security, control and recovery integrity.\n15. Why a read-only admin? Use cases:\nAuditors Monitoring systems Junior admins learning safely CI/CD validation jobs Read-only reduces accidental damage or malicious misuse.\n16. What\u0026rsquo;s the difference between a Linux user vs Proxmox user? Linux user → OS-level identity Proxmox user → Management-plane identity When using @pam, they are linked, but permissions are separate, auth happens via PAM, authorization happens via Proxmox RBAC.\nThat separation is what makes Proxmox enterprise-grade.\n","date":"February 7, 2026","hero":"/images/posts/opentofu.png","permalink":"https://siemforge.xyz/posts/orchestration/opentofu/opentofu1/","summary":"\u003ch1 id=\"securing-the-hypervisor-before-anything-else\"\u003eSecuring the hypervisor before anything else\u003c/h1\u003e\n\u003ch2 id=\"why-hypervisor-security-comes-first\"\u003eWhy hypervisor security comes first\u003c/h2\u003e\n\u003cp\u003eIn any virtualization environment, the hypervisor represents a high-value target. A compromised virtual machine is an incident. A compromised hypervisor is a catastrophe. Before building labs, automating infrastructure, or experimenting with offensive security techniques, it is essential to establish a hardened Proxmox VE baseline. This article walks through securing a single-node Proxmox installation as the foundation for a security-focused lab.\u003c/p\u003e","tags":null,"title":"OpenTofu Project: Part 1 - Proxmox foundations"},{"categories":null,"contents":"Executive summary Cato Networks logs stopped ingesting into Splunk following a customer-initiated unannounced maintenance. Investigation revealed that the API connection for log retrieval was interrupted, requiring manual intervention to restore service. The incident highlighted the impact of uncoordinated maintenance activities on log ingestion stability. Immediate checks were performed on the Heavy Forwarder logs, and the scripted input was reset to restore log flow. No other log sources were found to be affected at this time.\nData loss: No Cato Networks logs were ingested from maintenance until manual intervention, creating a monitoring gap. Logs were ingested using backlog. Only loss was visibility during the halted ingestion.\nManual recovery: Log ingestion was restored by restarting the scripted input, but the root cause was traced to uncoordinated system maintenance.\nLesson learned: Lack of communication regarding planned maintenance led to repeated manual fixes.\n1. Background Cato Networks logs are ingested into Splunk via a scripted input (python script EventFeed.py) on the Splunk Heavy Forwarder. After customer maintenance, log ingestion stopped, no new logs arrived in Splunk. We received an incident ticket about a missing logsource. The objective was to restore log ingestion, determine the root cause, and identify process improvements to prevent recurrence.\n2. Incident timeline Last log ingested: 2026-01-29T02:42:23Z Repeated failures: Similar interruptions occurred several times in the preceding days. Logs stopped ingesting around the same time (3am) but no steady pattern for past 4 days.\nTemporary fix: Disabling and re-enabling the scripted input which restored log flow.\n3. Troubleshooting steps 3.1. Log review Checked Splunk internal logs:\nindex=_internal log_level=ERROR source=\u0026#34;/opt/splunk/var/log/splunk/splunkd.log\u0026#34; cato Searched for errors related to Cato log ingestion and the scripted input, indicating API connection failures.\nConnectionResetError: [Errno 104] Connection reset by peer errors Checked system logs:\njournalctl | grep cato Used journalctl and Splunk’s internal logs to confirm no other log sources were affected.\n3.2. Scripted input management Splunk GUI: Disabled and re-enabled $SPLUNK_HOME/etc/apps/Splunk_Cato_TA/bin/script/cato.sh\nLog ingestion resumed.\n3.3. Root cause API connection interrupted: The API call for Cato Networks logs was interrupted by the maintenance, causing the script to fail.\nNo notification: The SIEM team was not informed of the planned maintenance, leading to manual recovery.\n4. Impact assessment Monitoring gap: No Cato Recycling logs were ingested for an extended period, reducing visibility for security monitoring.\nOperational Overhead: Manual intervention was required to restore service, increasing workload and response time.\nOther Log Sources: No evidence of issues with other log sources at this time, but continued monitoring is recommended.\n5. Lessons learned Importance of communication: Unannounced maintenance and upgrades can disrupt critical log ingestion processes. The SIEM team must be notified of any planned changes to infrastructure or log sources to coordinate and validate service continuity.\nProactive monitoring: Automated monitoring and alerting for log ingestion failures should be implemented to reduce reliance on manual detection and intervention.\nChange management: All changes to log collection infrastructure should follow a formal change management process, including stakeholder notification and post-change validation.\n","date":"February 2, 2026","hero":"/images/posts/cato.jpg","permalink":"https://siemforge.xyz/posts/siem/tech-reports/techreportcatonetworks/","summary":"\u003ch2 id=\"executive-summary\"\u003eExecutive summary\u003c/h2\u003e\n\u003cp\u003eCato Networks logs stopped ingesting into Splunk following a customer-initiated unannounced maintenance. Investigation revealed that the API connection for log retrieval was interrupted, requiring manual intervention to restore service. The incident highlighted the impact of uncoordinated maintenance activities on log ingestion stability. Immediate checks were performed on the Heavy Forwarder logs, and the scripted input was reset to restore log flow. No other log sources were found to be affected at this time.\u003c/p\u003e","tags":null,"title":"Tech report: Cato Networks log ingestion"},{"categories":null,"contents":"SIEM data onboarding: Best practices and a deepdive into Splunk vs Microsoft Sentinel Executive summary Security Information and Event Management (SIEM) platforms are only as effective as the data they receive. SIEM data onboarding is the process of getting the right logs, in the right format, into the SIEM so it can detect threats, support incident response, and demonstrate compliance. Simply “sending some logs” is not enough; quality, completeness, and normalization drive the value you get from your SIEM investment.\nSplunk and Microsoft Sentinel are two widely used SIEM platforms. They differ in how they ingest and manage data, but both require a disciplined onboarding approach. Splunk is highly flexible and can ingest virtually any data source with deep customization, while Sentinel is tightly integrated with Azure and Microsoft SaaS services, providing powerful native connectors. Neither tool will deliver value if onboarding is ad‑hoc or poorly governed.\nFrom a business perspective, good data onboarding:\nReduces risk by increasing visibility into identity, endpoint, and network activity.\nImproves incident response by enabling faster, more accurate investigation.\nControls cost by ensuring you ingest what you need, at the right volume and retention.\nSupports compliance with frameworks like ISO 27001:2022, NIST CSF 2.0, SOC 2 through auditable logging coverage.\nLeaders should treat SIEM data onboarding as a continuous program, not a one‑time project. It requires collaboration between the SOC, IT operations, application owners, and management. A clear onboarding strategy, supported by checklists and governance, will make Splunk or Sentinel significantly more effective and more cost‑efficient.\nIntroduction: Why data onboarding matters for SIEM effectiveness In a SIEM context, data onboarding is the end‑to‑end process of:\nConnecting log sources -\u0026gt; Parsing raw events -\u0026gt; Normalizing fields into a common schema -\u0026gt; Enriching with context (e.g., asset, user, threat intel) -\u0026gt; Validating completeness and quality\nThis is very different from “just sending logs” via syslog or an agent. Raw, unparsed logs may technically arrive in the SIEM, but: Fields won’t be extracted consistently -\u0026gt; Correlation rules won’t work reliably + Dashboards and KPIs will be wrong or misleading + Investigations will be slow and error‑prone.\nPoor onboarding leads directly to:\nMissed detections – key fields like user, IP, hostname, or action are missing or incorrect.\nAlert fatigue – noisy or low‑value logs generate overwhelming, low‑signal alerts.\nInaccurate reporting – compliance or management dashboards cannot be trusted.\nCompliance blind spots – you think you log critical systems, but the data is incomplete or misconfigured.\nBeginner note: If you’re new to SIEM, think of data onboarding as “teaching” your SIEM what your logs mean, so it can recognize suspicious behavior consistently across different systems.\nWhat is data onboarding and what are common log sources? Proper SIEM data onboarding includes the following stages:\nConnectivity: Agents, forwarders, APIs, syslog, cloud connectors.\nParsing: Extracting structured fields from raw text (e.g., source IP, user, action).\nNormalization: Mapping fields into a common schema/data model (e.g., “src_ip” vs “SourceIP”).\nEnrichment: Adding context like asset criticality, user roles, threat intel, geo‑IP.\nTagging: Applying tags such as authentication, network, endpoint for classification.\nValidation \u0026amp; monitoring: Verifying data volume, field population, error rates, and coverage over time.\nAzure AD logs Azure AD (Entra ID) logs provide insight into:\nUser sign‑ins and authentication events\nConditional access outcomes\nMFA success/failure\nApplication and resource access\nTypical security‑relevant fields:\nUser principal name (userPrincipalName)\nIP address / location\nApplication ID and client\nAuthentication result (success/failure)\nConditional access policies applied\nFirewall logs Firewall logs capture:\nAllowed and denied traffic\nSource and destination IPs and ports\nProtocol, action, and sometimes application identity\nTypical security‑relevant fields:\nsrc_ip, dest_ip, src_port, dest_port\nAction (allow/deny)\nRule name or policy ID\nBytes sent/received\nInterface or zone\nEDR logs Endpoint detection \u0026amp; response logs surface:\nProcess creation trees\nFile modifications and drops\nRegistry changes\nNetwork connections initiated by processes\nSecurity alerts from the EDR engine\nTypical security‑relevant fields:\nHostname / device ID\nUser context\nParent and child processes\nFile hash, path, and reputation\nAlert severity and category\nBeginner note: Azure AD, firewall, and EDR telemetry together give you visibility into identity, network, and endpoint, which are the three pillars of most modern detection strategies.\nCommon challenges and pitfalls in log onboarding 1. Incomplete log coverage Problem: Critical sources (e.g., Azure AD, key firewalls, production EDR) are not onboarded or only partially onboarded.\nImpact:\nBlind spots in authentication and identity‑based attacks.\nInability to reconstruct attack paths across network boundaries.\nGaps in incident timelines.\nExample: An account is compromised in Azure AD. Sign‑ins from unusual geo‑locations are logged, but Azure AD logs were never onboarded into the SIEM. The SOC only sees EDR alerts after the attacker reaches endpoints, missing early detection opportunities.\n2. Poor parsing and field extraction Problem: Logs are ingested as raw text; fields are not extracted consistently.\nImpact:\nDetection rules that rely on specific fields (e.g., src_ip) fail or are unreliable.\nDashboards show incorrect or aggregated values.\nExample: Firewall logs arrive as unparsed syslog messages. A correlation rule looking for repeated denied connections from the same IP fails because the src_ip field is not correctly extracted.\n3. Lack of normalization across tools Problem: Different vendors use different field names and formats, and no common schema is enforced.\nImpact:\nCorrelation rules must be rewritten per source.\nUse case content is not reusable.\nThreat hunting queries become very complex.\nExample: In Splunk, one data source uses src_ip, another uses source_ip. Sentinel has SrcIpAddr. Without a normalized model, a hunting query must account for all variants, increasing complexity and risk of errors.\n4. Incorrect time zones and host identification Problem: Time zones are misconfigured, or hostnames/IPs are inconsistent.\nImpact:\nIncident timelines are misleading.\nEvent correlation across systems breaks.\nExample: Firewall logs are in UTC, EDR logs in local time, Azure AD in another format. When the SOC reconstructs an attack, events appear out of order, suggesting impossible sequences and confusing investigators.\n5. High-volume, low-value logs Problem: High‑volume logs (e.g., verbose debug logs) are ingested without clear use cases.\nImpact:\nStorage and ingestion costs spike.\nNoise overwhelms detections.\nSOC time is wasted on low‑value data.\nExample: Verbose firewall “allow” logs are onboarded at full volume, but the SOC only has use cases for denied connections. Costs rise, but detection value does not.\n6. No documentation of onboarding decisions Problem: No clear record of which data sources are onboarded, why, and how.\nImpact:\nDifficult to prove coverage for audits.\nHard to evaluate gaps or onboard new sources.\nExample: An auditor asks for evidence that all critical identity systems are logged. The organization cannot easily show which identity sources are in the SIEM, for how long, and under which use cases.\nBest practices for onboarding logs into any SIEM 1. Prioritize critical sources Identity: Azure AD / Entra ID, on‑prem AD.\nEndpoint: EDR/XDR telemetry.\nNetwork perimeter: firewalls, VPNs, secure web gateways.\nCore infrastructure: domain controllers, critical servers.\n2. Define usecases before onboarding Identify key detection scenarios you want to support.\nMap them to required log sources and fields.\nOnboard only those logs that support real use cases or compliance needs.\n3. Establish a common event schema / data model Define standard field names and types (e.g., src_ip, dest_ip, user, hostname, action).\nAlign with:\nSplunk CIM categories (authentication, endpoint, network, etc.).\nSentinel tables and, where applicable, ASIM (Advanced Security Information Model).\nEnforce this across sources as much as feasible.\n4. Treat parsing, normalization, and enrichment as first‑class Invest in:\nParsers (regex, vendor‑provided add‑ons, connectors).\nEnrichment (asset CMDB, user identity, threat intel).\nDesign onboarding pipelines to include validation (field completeness, error logging).\nBeginner note: Normalization means different log formats are mapped into a consistent field set, so one query can work across many sources.\n5. Monitor data quality continuously Track:\nIngestion volume per source.\nPercentage of events with key fields populated.\nParser error rates.\nRegularly review:\nWhether logs still support current use cases.\nWhether cost aligns with value.\n**Beginner note – “Garbage in, garbage out”:**If raw logs are incomplete or poorly structured, your SIEM’s analytics and detections will be unreliable, no matter how advanced the tool is.\n6. Align with common frameworks (without legal guarantees) Map log coverage and onboarding practices to:\nISO 27001 Annex A controls related to logging and monitoring.\nNIST CSF Detect/respond functions.\nSOC 2 Security and availability criteria.\nAvoid “this guarantees compliance” language; instead say:\n“These practices support alignment with…” Splunk vs Microsoft Sentinel – Data onboarding workflows and examples Onboarding mechanisms Splunk:\nForwarders (Universal/Heavy) installed on servers or collectors.\nHTTP Event Collector (HEC) for custom or cloud applications.\nApps/Add‑ons from Splunkbase for specific products (e.g., Microsoft, firewalls, EDR).\nTypically involves:\nConfiguring input (e.g., inputs.conf).\nAssigning sourcetype, index, and metadata.\nApplying props/transforms for parsing and field extraction.\nMicrosoft Sentinel:\nBuilt on log analytics workspaces.\nData connectors for Azure services, Microsoft 365, and third parties.\nSupports:\nAzure agents / AMA.\nSyslog/CEF connectors.\nDirect API ingestion and custom logs.\nTypically involves:\nEnabling connector in Sentinel.\nConfiguring source (e.g., Azure subscription, M365 tenant, syslog server).\nMapping to relevant tables (e.g., SigninLogs, SecurityEvent, CommonSecurityLog).\na. Azure AD Logs – Onboarding and example queries Azure AD into Splunk Common approaches:\nSplunk Add‑on for Microsoft cloud services or similar.\nPulling sign‑in and audit logs via Microsoft Graph API.\nAssign sourcetype such as ms:azure:ad:signin.\nTypical fields (after parsing/normalization):\nuser_principal_name\nsrc_ip\nlocation\nresult (Success/Failure)\napp_display_name\nExample SPL – Detect multiple failed logins from multiple countries (impossible travel indicator):\nindex=azure_ad sourcetype=\u0026#34;ms:azure:ad:signin\u0026#34; result=\u0026#34;Failure\u0026#34; | stats dc(location_country) as distinct_countries, values(location_country) as countries, count as failed_attempts by user_principal_name, src_ip, _time | where distinct_countries \u0026gt; 1 AND failed_attempts \u0026gt; 5 | sort - failed_attempts This query looks for users with failed Azure AD sign‑ins from multiple countries, indicating potential impossible travel or credential stuffing.\nAzure AD into Sentinel Use native Azure AD / Entra ID connectors in Sentinel.\nData typically lands in SigninLogs and AuditLogs tables.\nExample KQL – Similar impossible travel logic:\nSigninLogs | where ResultType != 0 // non-success | summarize distinctCountries = dcount(LocationDetails.countryOrRegion), countries = make_set(LocationDetails.countryOrRegion), failedAttempts = count() by UserPrincipalName, IPAddress, bin(TimeGenerated, 1h) | where distinctCountries \u0026gt; 1 and failedAttempts \u0026gt; 5 | order by failedAttempts desc ` This KQL query identifies users with multiple failed sign‑ins from different countries within a given time window.\nb. Firewall logs – Onboarding and example queries Firewalls into Splunk Typical approach:\nSend syslog to a syslog collector.\nUse vendor‑specific Splunk Add‑on (e.g., for Palo Alto, Cisco ASA).\nAssign appropriate sourcetype (e.g., pan:traffic, cisco:asa).\nKey fields (post‑CIM mapping):\nsrc_ip, dest_ip\nsrc_port, dest_port\naction (allowed/blocked)\napp, rule\nExample SPL – Blocked outbound traffic to known bad IPs:\nindex=firewall sourcetype=\u0026#34;pan:traffic\u0026#34; action=\u0026#34;deny\u0026#34; | lookup threat_intel_bad_ips ip as dest_ip OUTPUTNEW category as threat_category | search threat_category=* | stats count by src_ip, dest_ip, threat_category, dest_port, app | sort - count This SPL identifies denied connections to IPs that appear in a threat intel list, grouped by source and destination.\nFirewalls into Sentinel Typical approach:\nUse Common Event Format (CEF) or Syslog connectors.\nLogs often end up in CommonSecurityLog or vendor‑specific tables.\nKey fields:\nSourceIP, DestinationIP\nSourcePort, DestinationPort\nAction\nDeviceAction, ApplicationProtocol\nExample KQL – Similar detection for blocked outbound traffic to known bad IPs:\nCommonSecurityLog | where DeviceAction == \u0026#34;deny\u0026#34; or DeviceAction == \u0026#34;blocked\u0026#34; | lookup kind=leftouter ThreatIntelIndicator on $left.DestinationIP == $right.NetworkIP | where isnotempty(ThreatType) | summarize count() by SourceIP, DestinationIP, ThreatType, DestinationPort, ApplicationProtocol | order by count_ desc This KQL correlates denied firewall connections with threat intelligence indicators to find possible exfiltration or C2 attempts.\nc. EDR logs – Onboarding and example Queries EDR into Splunk Typical approach:\nVendor‑specific Splunk apps/add‑ons or APIs (e.g., Microsoft Defender, CrowdStrike, etc.).\nEvents often assigned to endpoint category in CIM.\nKey fields:\nhost, user\nprocess_name, parent_process_name\nfile_path, file_hash\nevent_type (process_start, alert, network_connection)\nExample SPL – Suspicious process tree (e.g., Office spawning PowerShell):\nindex=edr sourcetype=\u0026#34;edr:process\u0026#34; event_type=\u0026#34;process_start\u0026#34; | where like(parent_process_name, \u0026#34;%winword.exe\u0026#34;) AND like(process_name, \u0026#34;%powershell.exe\u0026#34;) | stats count, values(file_path) as paths by host, user, parent_process_name, process_name | where count \u0026gt; 0 This SPL finds instances where Microsoft Word spawns PowerShell, a common indicator of macro‑based attacks.\nEDR into Sentinel Typical approach:\nUse native connectors, e.g., Microsoft Defender for Endpoint.\nLogs land in tables such as DeviceProcessEvents, DeviceEvents, SecurityAlert.\nExample KQL – Office spawning PowerShell:\nDeviceProcessEvents | where InitiatingProcessFileName in (\u0026#34;WINWORD.EXE\u0026#34;, \u0026#34;EXCEL.EXE\u0026#34;, \u0026#34;POWERPNT.EXE\u0026#34;) | where FileName =~ \u0026#34;powershell.exe\u0026#34; | summarize count(), any(InitiatingProcessCommandLine), any(ProcessCommandLine) by DeviceName, InitiatingProcessFileName, FileName, InitiatingProcessAccountName | where count_ \u0026gt; 0 This KQL identifies Office processes launching PowerShell on endpoints, a typical suspicious behavior pattern.\nData mapping and normalization (Field mappings, schemas, parsing, tuning) Splunk: CIM (Common Information Model) Splunk’s CIM defines standardized field names across domains:\nAuthentication\nEndpoint\nNetwork traffic\nWeb, malware, etc.\nAdd‑ons perform the heavy lifting:\nParse vendor‑specific log formats.\nMap fields to CIM.\nSentinel: Tables and ASIM Sentinel uses structured tables (e.g., SigninLogs, SecurityEvent, CommonSecurityLog, DeviceProcessEvents).\nASIM (Advanced Security Information Model) defines abstractions for certain data types (e.g., network sessions, DNS).\nExample: Azure AD field mapping Concept Splunk Field (CIM) Sentinel Field User principal name user / user_principal_name UserPrincipalName Source IP src_ip IPAddress Result result ResultType, ResultDescription Location countrysrc_country LocationDetails.countryOrRegion Query before vs after normalization Before normalization (Splunk – multiple Azure sources):\nindex=azure_ad OR index=legacy_identity | eval user=coalesce(user_principal_name, username, upn) | eval src_ip=coalesce(src_ip, ipAddress, client_ip) | search user=\u0026#34;alice@contoso.com\u0026#34; After normalization (Splunk – CIM aligned):\nindex=identity tag=authentication user=\u0026#34;alice@contoso.com\u0026#34; ` After mapping, queries become simpler and more reusable across sources.\nBefore normalization (Sentinel – different tables):\nSigninLogs | where UserPrincipalName == \u0026#34;alice@contoso.com\u0026#34; | union AuditLogs | where Identity == \u0026#34;alice@contoso.com\u0026#34; After ASIM/normalized view (conceptual example):\nimSignin | where User == \u0026#34;alice@contoso.com\u0026#34; (Assuming use of ASIM or custom views that provide a unified schema for sign‑in events.)\nParsing and tuning Key practices:\nAdjust field extractions/parsers:\nIn Splunk, tune props.conf and transforms.conf.\nIn Sentinel, ensure correct CEF mapping or custom parsing for CustomLogs.\nHandle noisy events:\nImplement filters at source or in ingestion rules.\nExclude verbose debug logs where no usecases exist.\nValidate key fields:\nRequire user, host, src_ip, dest_ip, action, and timestamp for key use cases.\nAlert on sudden drops in field population.\nRealistic scenarios / usecases 1. Compromised user account in Azure AD Narrative: An attacker obtains credentials for a user. They sign in from unusual locations, fail MFA repeatedly, and eventually succeed.\nCritical logs and fields:\nAzure AD / Entra ID:\nUserPrincipalName, IPAddress, LocationDetails.countryOrRegion, ResultType, AuthenticationRequirement EDR (later stages, lateral movement):\nDeviceName, process events, alerts Splunk SPL – Detect suspicious sign‑ins followed by success:\nindex=azure_ad sourcetype=\u0026#34;ms:azure:ad:signin\u0026#34; | eval result=if(ResultType==0,\u0026#34;Success\u0026#34;,\u0026#34;Failure\u0026#34;) | transaction user_principal_name maxspan=1h startswith=(result=\u0026#34;Failure\u0026#34;) endswith=(result=\u0026#34;Success\u0026#34;) | where like(countries, \u0026#34;%Unknown%\u0026#34;) OR mvcount(countries) \u0026gt; 1 | table _time, user_principal_name, countries, src_ip, result This SPL looks for sequences of failed then successful sign‑ins within an hour, from varying or unknown locations.\nSentinel KQL – Similar pattern:\nlet FailedLogons = SigninLogs | where ResultType != 0 | project UserPrincipalName, IPAddress, Location = LocationDetails.countryOrRegion, TimeGenerated; let SuccessfulLogons = SigninLogs | where ResultType == 0 | project UserPrincipalName, IPAddress, Location = LocationDetails.countryOrRegion, TimeGenerated; FailedLogons | join kind=inner (SuccessfulLogons) on UserPrincipalName | where SuccessfulLogons_TimeGenerated between (FailedLogons_TimeGenerated .. FailedLogons_TimeGenerated + 1h) | where FailedLogons_Location != SuccessfulLogons_Location This KQL finds users with failed then successful logons within 1 hour from different locations.\n2. Suspicious outbound firewall traffic (potential exfiltration) Narrative: A compromised host starts sending large volumes of outbound traffic to unknown, potentially malicious IPs over unusual ports.\nCritical logs and fields:\nFirewall:\nsrc_ip, dest_ip, dest_port, bytes_out, action EDR (optional):\nProcess initiating connections Threat intelligence:\nKnown bad or suspicious IPs/domains Splunk SPL – High volume denied outbound traffic:\nindex=firewall action=\u0026#34;deny\u0026#34; | stats sum(bytes_out) as total_bytes, count as event_count by src_ip, dest_ip, dest_port | where total_bytes \u0026gt; 10000000 OR event_count \u0026gt; 1000 | sort - total_bytes This SPL identifies source IPs sending large amounts of denied outbound traffic, which might indicate exfiltration attempts.\nSentinel KQL – Similar detection:\nCommonSecurityLog | where DeviceAction in (\u0026#34;deny\u0026#34;, \u0026#34;block\u0026#34;) | summarize totalBytes = sum(toint(BytesSent)), eventCount = count() by SourceIP, DestinationIP, DestinationPort | where totalBytes \u0026gt; 10000000 or eventCount \u0026gt; 1000 | order by totalBytes desc This KQL finds blocked outbound connections with unusually high traffic volume.\n3. Post‑exploitation activity in EDR logs Narrative: After gaining a foothold, an attacker executes malicious tools and uses lateral movement techniques from compromised endpoints.\nCritical logs and fields:\nEDR:\nProcess creation, network connections, alerts, user context. Azure AD:\nPossible concurrent abnormal sign‑ins. Firewall:\nEast‑west traffic, SMB/RDP, unusual ports. Splunk SPL – Hunting for suspicious tools (e.g., psexec, wmic):\nindex=edr sourcetype=\u0026#34;edr:process\u0026#34; event_type=\u0026#34;process_start\u0026#34; | search process_name IN (\u0026#34;psexec.exe\u0026#34;,\u0026#34;wmic.exe\u0026#34;,\u0026#34;wmiprvse.exe\u0026#34;) | stats count, values(parent_process_name) as parents, values(file_path) as paths by host, user, process_name | where count \u0026gt; 3 This SPL hunts for repeated execution of lateral movement tools across hosts.\nSentinel KQL – Similar EDR hunt:\nDeviceProcessEvents | where FileName in (\u0026#34;psexec.exe\u0026#34;, \u0026#34;wmic.exe\u0026#34;, \u0026#34;wmiprvse.exe\u0026#34;) | summarize count(), parentProcs = make_set(InitiatingProcessFileName), paths = make_set(FolderPath) by DeviceName, InitiatingProcessAccountName, FileName | where count_ \u0026gt; 3 This KQL finds frequent use of tools commonly associated with lateral movement.\nIn all cases, success or failure of these detections depends heavily on:\nAzure AD, firewall, and EDR logs being properly onboarded.\nKey fields (user, IP, host, process) being correctly parsed and normalized.\nEnsuring time synchronization and a unified schema.\nCost, scalability, and operational aspects of ingestion Conceptual cost models Splunk:\nTypically licenses based on ingested data volume (and/or infrastructure/host‑based models, depending on edition).\nHigh‑volume logs have direct cost implications.\nSentinel:\nBuilt on Azure Log Analytics; pricing is largely per GB of data ingested and retained, with options for reserved capacity and archive tiers. Avoid relying on specific pricing numbers: refer to official vendor documentation, as models and rates may change.\nRetention and storage Splunk:\nHot/warm/cold/frozen buckets; data roll‑off and archival controlled by index settings. Sentinel:\nConfigurable retention per workspace/table, with options for long‑term archive and search. Strategies to reduce cost without losing value Prioritize must‑have vs nice‑to‑have logs.\nFilter unneeded events at source (e.g., debug logs).\nUse summaries or metrics where raw detail is not always required.\nUse differential retention:\nShorter retention for high‑volume, low‑value logs.\nLonger retention for critical security and compliance data.\nScalability and operations Splunk:\nScale out indexers and search heads.\nRequires capacity planning for on‑prem or IaaS deployments.\nSentinel:\nBuilt on Azure infrastructure; scaling is largely handled by the platform.\nStill requires planning around workspace design, regionality, and performance.\nPractical tips Regularly review ingestion by:\nIndex / table\nLog source type\nUse case mapping\nInvolve finance / cost management teams to align SIEM ingestion strategy with budget and risk appetite.\nStakeholder informing and collaboration SIEM data onboarding is not just for the SOC; it involves multiple stakeholders:\nSOC / Security engineering – defines usecases and detection logic.\nIT operations / Infrastructure – implements connectors, forwarders, agents.\nApplication owners – provide understanding of application logs and criticality.\nSecurity leadership / CISO – sets priorities and risk tolerance.\nCompliance / Risk – aligns logging with regulatory or policy requirements.\nCommunicating with managers and C‑suite Frame onboarding in terms of:\nRisk reduction and incident response readiness.\nCost vs. visibility – what business risks are mitigated by specific log sources.\nCompliance posture and audit readiness.\nSuccess metrics Percentage of critical systems with fully onboarded logging.\nCoverage of high‑priority use cases.\nTime‑to‑detect and time‑to‑respond metrics.\nReduction in false positives due to better normalization and tuning.\nSimple stakeholder communication template You can use a simple table or slide format like:\nItem Description Business objective e.g., Detect account takeover in Azure AD Critical log sources Azure AD, EDR, VPN, firewall Onboarding status Azure AD \u0026amp; EDR onboarded; VPN in progress Key fields required User, IP, device, location, MFA result Cost considerations Est. X GB/day; options to filter low-value events Risks if not onboarded Missed early compromise, delayed incident response Next steps \u0026amp; owners SOC + Identity team to finalize onboarding by \u0026lt;date\u0026gt; Practical SIEM data onboarding checklist Use this tool‑neutral checklist to drive your program. Add Splunk/Sentinel specifics where appropriate.\nIdentify critical log sources\nAzure AD / Entra ID\nEDR/XDR\nFirewalls and VPNs\nDomain controllers and key infrastructure\nSplunk note: Map each to appropriate indexes and sourcetype. Sentinel note: Map each to correct connectors and tables.\nDefine detection usecases\nList primary threats (e.g., account takeover, lateral movement, exfiltration).\nMap each threat to required log sources and fields.\nPrioritize based on risk and feasibility.\nDesign normalization and data model\nDefine standard field names (user, host, src_ip, dest_ip, action).\nFor Splunk: align with relevant CIM data models.\nFor Sentinel: align with table schemas and ASIM where applicable.\nDocument mappings per source.\nConfigure and test ingestion\nSet up connectors/forwarders/agents.\nValidate data arrives in expected index/table.\nVerify parsers extract all required fields.\nConfirm timestamps and time zones are correct.\nValidate fields \u0026amp; data quality\nCheck % of events with key fields populated.\nValidate event volumes vs expectations.\nMonitor parser errors and fix recurring issues.\nTune for noise and cost\nFilter out low‑value events where no use cases exist.\nAdjust retention by source and index/table.\nPeriodically review ingestion vs detected incidents.\nDocument and govern\nMaintain a central inventory of onboarded sources.\nRecord purpose, use cases, and data mappings.\nReview coverage and priorities quarterly with stakeholders.\nIterate and improve\nRevisit use cases as the threat landscape evolves.\nValidate detection performance and adjust data onboarding accordingly.\nAlign improvements with ISO 27001, NIST CSF, and SOC 2 where relevant (without treating them as guarantees).\nConclusion High‑quality SIEM data onboarding is the foundation of effective detection, response, and compliance. Splunk and Microsoft Sentinel both offer powerful capabilities, but they differ in how data is ingested, normalized, and managed:\nSplunk provides extreme flexibility through forwarders, HEC, and rich add‑ons, well‑suited for heterogeneous environments.\nSentinel provides deep native integration with Azure and Microsoft SaaS, simplifying onboarding for organizations heavily invested in the Microsoft ecosystem.\nNeither platform can compensate for poor data onboarding practices. If critical sources like Azure AD, firewall, and EDR are not properly parsed, normalized, and validated, your SIEM will miss threats, generate noise, and provide questionable reports.\nA disciplined onboarding strategy—supported by a common schema, good parsers, continuous tuning, and cross‑stakeholder collaboration—turns Splunk or Sentinel into a reliable detection and investigation platform instead of an expensive log bucket.\nNext steps: Engage SOC, IT, and leadership teams in a structured onboarding plan that balances risk, visibility, and cost.\nFAQ 1. What is SIEM data onboarding?\nSIEM data onboarding is the process of connecting log sources to a SIEM, parsing and normalizing events into a common schema, enriching them with context, and validating data quality so detections and investigations work reliably.\n2. Why is data onboarding important for SIEM effectiveness?\nWithout proper onboarding, key fields may be missing or inconsistent, causing missed detections, noisy alerts, and unreliable dashboards. Good onboarding turns raw logs into actionable security telemetry.\n3. How do Splunk and Microsoft Sentinel differ in data onboarding?\nSplunk uses forwarders, HEC, and add‑ons for flexible ingestion across many environments. Sentinel relies on Log Analytics‑based connectors tightly integrated with Azure and Microsoft 365. Both require proper parsing and normalization to be effective.\n4. Which log sources should I onboard first to my SIEM?\nTypically start with identity (Azure AD/Entra ID, AD), endpoint (EDR), and network perimeter (firewalls, VPN). These sources provide the core visibility needed to detect account compromise, lateral movement, and data exfiltration.\n5. What is log normalization in SIEM?\nNormalization maps different vendor log formats into a common set of fields and schemas. This allows you to write reusable detection rules and hunting queries that work across multiple data sources.\n6. How can I control SIEM ingestion costs without losing detection value?\nPrioritize high‑value sources, filter low‑value events, tune verbose logs, apply tiered retention, and regularly review ingestion against your active detections and use cases. Both Splunk and Sentinel support strategies to balance cost with visibility.\n","date":"January 16, 2026","hero":"/images/posts/siem.jpg","permalink":"https://siemforge.xyz/posts/siem/data-onboarding/dataonboardingpost/","summary":"\u003ch1 id=\"siem-data-onboarding-best-practices-and-a-deepdive-into-splunk-vs-microsoft-sentinel\"\u003eSIEM data onboarding: Best practices and a deepdive into Splunk vs Microsoft Sentinel\u003c/h1\u003e\n\u003ch2 id=\"executive-summary\"\u003eExecutive summary\u003c/h2\u003e\n\u003cp\u003eSecurity Information and Event Management (SIEM) platforms are only as effective as the data they receive. SIEM data onboarding is the process of getting the right logs, in the right format, into the SIEM so it can detect threats, support incident response, and demonstrate compliance. Simply “sending some logs” is not enough; quality, completeness, and normalization drive the value you get from your SIEM investment.\u003c/p\u003e","tags":null,"title":"SIEM data onboarding"},{"categories":null,"contents":"TCPDump intro for Linux engineers As a SIEM platform administrator, one of the most invaluable tools in my troubleshooting arsenal is tcpdump. It allows the user to display TCP/IP and other packets being transmitted or received over a network. Despite its simplicity, it is incredibly powerful for debugging complex network issues, monitoring traffic, or simply learning how different protocols behave.\nWhy use tcpdump Real-time packet inspection Lightweight and scriptable No need for a GUI Useful for security auditing and troubleshooting How to install tcpdump On Debian/Ubuntu-based systems:\nsudo apt update \u0026amp;\u0026amp; sudo apt install tcpdump On RHEL/CentOS (YUM-based systems):\nsudo yum install tcpdump On Fedora/DNF-based systems:\nsudo dnf install tcpdump From Source:\nsudo apt install libpcap-dev gcc make -y wget http://www.tcpdump.org/release/tcpdump-4.99.4.tar.gz tar -xvzf tcpdump-4.99.4.tar.gz cd tcpdump-4.99.4 ./configure make sudo make install Basic usage examples Capture traffic on eth0\ntcpdump -i eth0 Capture only TCP packets\ntcpdump -i eth0 tcp Capture packets from a specific host\ntcpdump -i eth0 host 192.168.1.100 Save captured data to a file\ntcpdump -i eth0 -w capture.pcap Read from a saved capture file\ntcpdump -r capture.pcap Limit the number of packets\ntcpdump -i eth0 -c 100 Filter by port\ntcpdump -i eth0 port 443 Verbose output with hex\ntcpdump -XX -i eth0 Display packet timestamp in readable format\ntcpdump -tttt -i eth0 Real-World Example: Debugging HTTP Requests\ntcpdump -i eth0 -nn -s 0 -v tcp port 80 This captures HTTP requests in verbose mode, disables hostname and port name resolution, and captures the entire packet.\nSecurity consideration You need elevated privileges to run tcpdump. Consider creating a dedicated group or using capabilities:\nsudo setcap cap_net_raw,cap_net_admin=eip $(which tcpdump) Important notice! During troubleshooting, I encountered an issue on two occasions where no logs were received. Further investigation with tcpdump revealed insights into the packets that were received. When comparing incoming data, it was observed that the data had a length greater than zero, whereas the other log source only showed packets of length zero. After restarting the syslog service on the host that stopped sending data, new data was successfully ingested into our SIEM.\nPlease note that a packet length of zero can indicate two possibilities: the data might be encrypted, or, as in the example provided, the packets could be used to maintain the connection (keep-alive packets).\nConclusion Whether you\u0026rsquo;re a network administrator, security professional, or developer, tcpdump is a skill worth mastering. Its ability to dissect network traffic at the most granular level makes it indispensable in any Linux environment.\nCheatsheet A more extensive cheatsheet can be found here: TCPDump cheatsheet\n","date":"June 24, 2025","hero":"/images/posts/tcpdump.png","permalink":"https://siemforge.xyz/posts/linux/tcpdump/tcpdumppost/","summary":"\u003ch1 id=\"tcpdump-intro-for-linux-engineers\"\u003eTCPDump intro for Linux engineers\u003c/h1\u003e\n\u003cp\u003eAs a SIEM platform administrator, one of the most invaluable tools in my troubleshooting arsenal is tcpdump. It allows the user to display TCP/IP and other packets being transmitted or received over a network. Despite its simplicity, it is incredibly powerful for debugging complex network issues, monitoring traffic, or simply learning how different protocols behave.\u003c/p\u003e\n\u003ch2 id=\"why-use-tcpdump\"\u003eWhy use tcpdump\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eReal-time packet inspection\u003c/li\u003e\n\u003cli\u003eLightweight and scriptable\u003c/li\u003e\n\u003cli\u003eNo need for a GUI\u003c/li\u003e\n\u003cli\u003eUseful for security auditing and troubleshooting\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"how-to-install-tcpdump\"\u003eHow to install tcpdump\u003c/h2\u003e\n\u003cp\u003eOn Debian/Ubuntu-based systems:\u003c/p\u003e","tags":null,"title":"TCPDump"},{"categories":null,"contents":"First start with LVM (Logical Volume Manager) What is LVM? LVM stands for Logical Volume Manager, a device mapper framework that provides logical volume management for the Linux kernel. LVM allows administrators to create, resize, and delete volumes dynamically, offering more flexibility than traditional partitioning schemes.\nWhy use LVM? Dynamic resizing: Easily resize (extend/reduce) logical volumes without unmounting. Snapshot support: Create point-in-time snapshots for backup or testing. Volume grouping: Aggregate multiple physical devices into a single storage pool. Migration: Move volumes across physical devices live. Flexibility: Logical volumes can span across multiple disks. LVM architecture Physical Volume (PV): Raw physical storage (disk, partition, etc). Volume Group (VG): Pool of storage formed by combining multiple PVs. Logical Volume (LV): Virtual partition formed from a part of the VG. Physical Extents (PE): Smallest unit of storage on a PV, a block within the partition/volume Logical Extents (LE): Mapped 1:1 to PEs for simplicity. Comparison: LVM vs traditional partitioning Feature LVM Traditional Partitioning Resizing Volumes Dynamic Static Snapshots Yes No Disk Spanning Yes (via VGs) No Performance Overhead Slight (negligible in most) None Complexity Moderate Low Alternatives ZFS: Advanced file system with built-in volume management and redundancy. Btrfs: Modern Linux filesystem with snapshot and RAID features. Traditional partitioning: Simple, low overhead, but limited flexibility. Usecase: LVM for database storage A PostgreSQL database needs fast backups and storage flexibility. With LVM:\nCreate an LV for the database. Use lvcreate --size 10G. Take snapshots before major upgrades: lvcreate --snapshot. Extend storage as DB grows with lvextend. Important! Extending a logical volume increases the underlying block device size, but the filesystem itself must also be explicitly resized to utilize the newly allocated space. Without this step, the additional storage remains unavailable to the operating system and applications.\nXFS\nXFS filesystems must be mounted to resize. Use xfs_growfs on the mount point\nxfs_growfs /path/to/your/mount/point. ext filesystems\nFor ext-based filesystems (ext2/3/4), you can resize the filesystem either by targeting the block device or the mount point (if already mounted)\nresize2fs /dev/vg_name/lv_name # or resize2fs /mount/point Cheatsheet A more extensive cheatsheet can be found here: LVM cheatsheet\n","date":"June 22, 2025","hero":"/images/posts/lvmlogo.png","permalink":"https://siemforge.xyz/posts/linux/lvm/lvmpost/","summary":"\u003ch1 id=\"first-start-with-lvm-logical-volume-manager\"\u003eFirst start with LVM (Logical Volume Manager)\u003c/h1\u003e\n\u003ch2 id=\"what-is-lvm\"\u003eWhat is LVM?\u003c/h2\u003e\n\u003cp\u003eLVM stands for \u003cstrong\u003eLogical Volume Manager\u003c/strong\u003e, a device mapper framework that provides logical volume management for the Linux kernel. LVM allows administrators to create, resize, and delete volumes dynamically, offering more flexibility than traditional partitioning schemes.\u003c/p\u003e\n\u003ch2 id=\"why-use-lvm\"\u003eWhy use LVM?\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDynamic resizing\u003c/strong\u003e: Easily resize (extend/reduce) logical volumes without unmounting.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSnapshot support\u003c/strong\u003e: Create point-in-time snapshots for backup or testing.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVolume grouping\u003c/strong\u003e: Aggregate multiple physical devices into a single storage pool.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMigration\u003c/strong\u003e: Move volumes across physical devices live.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFlexibility\u003c/strong\u003e: Logical volumes can span across multiple disks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"lvm-architecture\"\u003eLVM architecture\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/posts/lvm.png\" alt=\"LVM Architecture\"\u003e\u003c/p\u003e","tags":null,"title":"LVM"},{"categories":null,"contents":"My first steps into automation with AI agents using n8n Setting the stage: n8n in my homelab As someone constantly tinkering in my Proxmox-based homelab, I recently decided to explore workflow automation with AI agents using n8n — a powerful, self-hostable automation tool that connects anything to everything.\nTo get started, I spun up a new container and installed n8n via Portainer, running it in Docker. For anyone looking to do the same, here\u0026rsquo;s the official n8n install guide for Docker and Portainer:\nInstall n8n with Docker \u0026amp; Portainer\nLink to environment variables\nTrial and error: The hands-on approach Initially, I went in completely blind. I began dragging in random nodes, loosely configuring them, and hoped for some magic.\nSpoiler: it didn’t work.\nI kept hitting walls — nodes not connecting, credentials misbehaving, and the dreaded “nothing happened” moments. But each small failure taught me a bit more about how n8n thinks and how its workflows execute.\n💡 The language learning idea While experimenting, I had a lightbulb moment: why not use AI to help me learn a new language?\nHere was the concept:\nSchedule a daily trigger Use an AI agent to ask a chatmodel (e.g., Google Gemini) for a new sentence in the target language Include translation, pronunciation, grammar, and spelling tips Send the output to a Discord channel via webhook This felt like the perfect fusion of AI, automation, and a personal need.\nTurning to tutorials Realizing I needed a stronger foundation, I dove into some n8n tutorials:\nYour First n8n Workflow\nThis tutorial helped me understand how to structure a simple workflow from trigger to output.\nIntro to Advanced AI Workflows\nThis one introduced the concept of using AI models within workflows — exactly what I needed.\nAfter going through these, things started to click.\nThe first major roadblock I created my workflow: a Schedule Trigger connected to an AI agent and ending in a Discord webhook. It worked beautifully when I ran it manually, but… nothing happened when the scheduled time passed.\nProblem: The Schedule Trigger didn’t fire on its own.\nSolution: My Docker container was running, but n8n wasn’t in active/production mode. I updated my Docker container\u0026rsquo;s environment variables: N8N_EXECUTIONS_MODE=own TZ=Europe/Brussels (to match my timezone) I also switched the trigger from interval to cron (45 9 * * *) for reliability. Finally, I ensured the workflow was activated and saved. After these changes, the scheduled execution worked like a charm.\nEarly results \u0026amp; simplifying things Initially, I tried to go way too complex — chaining advanced AI nodes, formatting logic, and conditionals. Eventually, I scaled it back to something simpler just to start getting data flowing and build confidence.\nThe minimal setup: Schedule Trigger → AI Agent → Discord Webhook\nAn AI agent is not fully required as this could have been configured as an automation workflow, this was purely for testing purposes. This project provided value and a great learning experience.\nNext steps \u0026amp; ideas Here’s where I want to take this next:\nInteractive Discord bot: Respond to messages in a channel, maybe even answer grammar questions. Clean-up logic: Automatically delete older language messages to keep the channel fresh. Add memory node: Track which sentences have already been sent and avoid duplicates (or regenerate). Logging to Google Sheets: Keep a log of daily sentences and explanations for review. Final thoughts Exploring automation with n8n + AI has been one of the most satisfying and productive rabbit holes I’ve gone down in my homelab journey. With a bit of trial and error, and help from the excellent n8n docs and community, I now have a working system that automates part of my language learning journey — and that’s just the beginning.\nIf you’re on the fence about diving into automation or AI integration, my advice? Just start — even the mistakes are great teachers.\nWant to see more of my homelab experiments? Let me know — happy to keep sharing!\n","date":"June 18, 2025","hero":"/images/posts/n8n.jpg","permalink":"https://siemforge.xyz/posts/automation/n8n/firstautomationworkflow/","summary":"\u003ch1 id=\"my-first-steps-into-automation-with-ai-agents-using-n8n\"\u003eMy first steps into automation with AI agents using n8n\u003c/h1\u003e\n\u003ch2 id=\"setting-the-stage-n8n-in-my-homelab\"\u003eSetting the stage: n8n in my homelab\u003c/h2\u003e\n\u003cp\u003eAs someone constantly tinkering in my \u003cstrong\u003eProxmox-based homelab\u003c/strong\u003e, I recently decided to explore \u003cstrong\u003eworkflow automation with AI agents\u003c/strong\u003e using \u003ca href=\"https://n8n.io/\" target=\"_blank\" rel=\"noopener\"\u003en8n\u003c/a\u003e — a powerful, self-hostable automation tool that connects anything to everything.\u003c/p\u003e\n\u003cp\u003eTo get started, I spun up a new container and installed \u003cstrong\u003en8n via Portainer\u003c/strong\u003e, running it in Docker. For anyone looking to do the same, here\u0026rsquo;s the official n8n install guide for Docker and Portainer:\u003c/p\u003e","tags":null,"title":"My first automation workflow"},{"categories":null,"contents":"Mastering Logrotate: The unsung hero of log management In the trenches of system administration, there\u0026rsquo;s one silent guardian that keeps your disk space from imploding under a mountain of logs: Logrotate. Whether you\u0026rsquo;re wrangling logs on a sprawling Kubernetes cluster or just babysitting a single Linux box, Logrotate ensures your logs don’t spiral out of control.\nWe’ll explore how Logrotate works, why it’s essential, how to configure it like a pro, and how to troubleshoot when it throws a tantrum.\nWhat is Logrotate? At its core, Logrotate is a Unix utility designed to automatically rotate, compress, and remove log files. Think of it as your log janitor — sweeping away old logs, zipping them up neatly, and ensuring your system never runs out of space due to runaway logging.\nWhy it matters: Imagine you’re running NGINX on a production server handling thousands of requests per second. Without log rotation, your access and error logs could balloon into gigabytes overnight. That’s a disk I/O nightmare waiting to happen.\nLogrotate configuration: From basics to battle-hardened Logrotate’s power lies in its flexibility. You can define global rules in /etc/logrotate.conf, or per-service rules in /etc/logrotate.d/.\nExample: /etc/logrotate.d/nginx /var/log/nginx/*.log { daily rotate 14 compress delaycompress missingok notifempty create 0640 www-data adm sharedscripts postrotate systemctl reload nginx \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 || true endscript } What each directive does: daily: Rotate logs every day (alternatives: weekly, monthly, or custom intervals).\nrotate 14: Keep 14 archived logs before purging.\ncompress: Gzip old logs to save space.\ndelaycompress: Wait a cycle before compressing (avoids compressing still-used logs).\nmissingok: Don’t freak out if a log file is missing.\nnotifempty: Skip rotation for empty logs.\ncreate 0640 www-data adm: Create new logs with specific permissions.\nsharedscripts: Ensures postrotate only runs once per log group.\npostrotate...endscript: What to do after rotating logs — in this case, gracefully reload NGINX.\nPro Tip: Use copytruncate if your app won’t release the log file handle — useful for apps that don\u0026rsquo;t support log reopening on SIGHUP.\nAutomating with cron Logrotate is typically triggered by cron, so unless you\u0026rsquo;re into manual rotations (why?), make sure this line exists in your cron jobs:\n0 0 * * * /usr/sbin/logrotate /etc/logrotate.conf \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 This runs it daily at midnight. You can customize it further with anacron or systemd timers if you\u0026rsquo;re using newer systems like RHEL 8+ or Ubuntu 22.04+.\nCommon pitfalls and how to debug like a boss Even a veteran sysadmin can get tripped up by Logrotate quirks. Here are some frequent headaches and their antidotes:\nProblem\tfix 🔁 Log not rotating?\tCheck if the cron job runs. Use logrotate -d for dry-run debugging.\n🔒 Permission denied?\tEnsure Logrotate has access (run as root or use sudo).\n❌ Syntax error in config?\tRun logrotate -v /etc/logrotate.conf to see what it’s doing.\n💾 Disk still filling up?\tReduce rotate count or increase compression aggressiveness.\n🔄 App not releasing log files?\tUse copytruncate or restart the service post-rotate.\nDocker Running containers? Docker log files can balloon fast under /var/lib/docker/containers/. You can use logrotate or — better — configure Docker\u0026rsquo;s own json-file driver like this:\n{ \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;50m\u0026#34;, \u0026#34;max-file\u0026#34;: \u0026#34;3\u0026#34; } } Boom — Docker will auto-rotate logs per container.\nFinal thoughts Logrotate might not be flashy, but it’s a critical piece of your system\u0026rsquo;s hygiene. Treat it like a core dependency — because it is. With smart configurations, regular checks, and a few advanced tweaks, you can keep your logs lean, searchable, and under control.\nWhether you\u0026rsquo;re taming Apache logs, managing containerized chaos, or just keeping your home lab in shape, Logrotate will be your quiet, powerful ally.\nTL;DR cheat sheet Config files: /etc/logrotate.conf, /etc/logrotate.d/*\nCommon flags: daily, rotate, compress, create, copytruncate\nDebug command: logrotate -d /etc/logrotate.conf\nTest manually: logrotate -f /etc/logrotate.conf\nKeep it cron’d: crontab -e\nA more extensive cheatsheet can be found here: Logrotate cheatsheet\nNow go forth and rotate like a boss. 🌀\n","date":"June 6, 2025","hero":"/images/posts/logrotate.jpg","permalink":"https://siemforge.xyz/posts/linux/logrotate/logrotatepost/","summary":"\u003ch1 id=\"mastering-logrotate-the-unsung-hero-of-log-management\"\u003eMastering Logrotate: The unsung hero of log management\u003c/h1\u003e\n\u003cp\u003eIn the trenches of system administration, there\u0026rsquo;s one silent guardian that keeps your disk space from imploding under a mountain of logs: \u003cstrong\u003eLogrotate\u003c/strong\u003e. Whether you\u0026rsquo;re wrangling logs on a sprawling Kubernetes cluster or just babysitting a single Linux box, Logrotate ensures your logs don’t spiral out of control.\u003c/p\u003e\n\u003cp\u003eWe’ll explore how Logrotate works, why it’s essential, how to configure it like a pro, and how to troubleshoot when it throws a tantrum.\u003c/p\u003e","tags":null,"title":"Logrotate"}]